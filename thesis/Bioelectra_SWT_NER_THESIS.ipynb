{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Dataset' from 'datasets' (c:\\Users\\Gbadamosi\\Documents\\Nerd Corner\\Master in ds and AI\\MSC project\\workspace\\workspace\\ner\\notebooks\\tests codes\\datasets.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gbadamosi\\Documents\\Nerd Corner\\Master in ds and AI\\MSC project\\workspace\\workspace\\ner\\notebooks\\tests codes\\Bioelectra_SWT_NER_THESIS.ipynb Cell 1\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gbadamosi/Documents/Nerd%20Corner/Master%20in%20ds%20and%20AI/MSC%20project/workspace/workspace/ner/notebooks/tests%20codes/Bioelectra_SWT_NER_THESIS.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gbadamosi/Documents/Nerd%20Corner/Master%20in%20ds%20and%20AI/MSC%20project/workspace/workspace/ner/notebooks/tests%20codes/Bioelectra_SWT_NER_THESIS.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Gbadamosi/Documents/Nerd%20Corner/Master%20in%20ds%20and%20AI/MSC%20project/workspace/workspace/ner/notebooks/tests%20codes/Bioelectra_SWT_NER_THESIS.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mevaluate\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gbadamosi/Documents/Nerd%20Corner/Master%20in%20ds%20and%20AI/MSC%20project/workspace/workspace/ner/notebooks/tests%20codes/Bioelectra_SWT_NER_THESIS.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gbadamosi/Documents/Nerd%20Corner/Master%20in%20ds%20and%20AI/MSC%20project/workspace/workspace/ner/notebooks/tests%20codes/Bioelectra_SWT_NER_THESIS.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\virtual environments\\natural_language_1.0\\Lib\\site-packages\\evaluate\\__init__.py:29\u001b[0m\n\u001b[0;32m     25\u001b[0m SCRIPTS_VERSION \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m version\u001b[39m.\u001b[39mparse(__version__)\u001b[39m.\u001b[39mis_devrelease \u001b[39melse\u001b[39;00m __version__\n\u001b[0;32m     27\u001b[0m \u001b[39mdel\u001b[39;00m version\n\u001b[1;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mevaluation_suite\u001b[39;00m \u001b[39mimport\u001b[39;00m EvaluationSuite\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mevaluator\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     31\u001b[0m     AudioClassificationEvaluator,\n\u001b[0;32m     32\u001b[0m     AutomaticSpeechRecognitionEvaluator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     evaluator,\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mhub\u001b[39;00m \u001b[39mimport\u001b[39;00m push_to_hub\n",
      "File \u001b[1;32mc:\\virtual environments\\natural_language_1.0\\Lib\\site-packages\\evaluate\\evaluation_suite\\__init__.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Callable, Dict, Optional, Union\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset, DownloadConfig, DownloadMode, load_dataset\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m Version\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevaluator\u001b[39;00m \u001b[39mimport\u001b[39;00m evaluator\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Dataset' from 'datasets' (c:\\Users\\Gbadamosi\\Documents\\Nerd Corner\\Master in ds and AI\\MSC project\\workspace\\workspace\\ner\\notebooks\\tests codes\\datasets.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "COLAB = 'google.colab' in sys.modules\n",
    "if COLAB:\n",
    "    !nvidia-smi\n",
    "COLAB\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512,garbage_collection_threshold:0.9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tru_label = ['O',\n",
    "              'B-cond', \n",
    "              'I-cond',\n",
    "              'B-des',\n",
    "              'I-des',\n",
    "              'B-subj',\n",
    "              'I-subj',\n",
    "              \"B-group_A\", \n",
    "              \"I-group_A\", \n",
    "              \"B-group_B\", \n",
    "              \"I-group_B\", \n",
    "              \"B-group_C\",\n",
    "              \"I-group_C\",\n",
    "              'B-group_D',\n",
    "              'I-group_D', \n",
    "              'B-proc',\n",
    "              'I-proc', \n",
    "              'B-N_A',\n",
    "              'I-N_A',\n",
    "              'B-N_B',\n",
    "              'I-N_B',\n",
    "              'B-ther',\n",
    "              'I-ther']\n",
    "\n",
    "id2label = {i:label for i, label in enumerate(tru_label) }\n",
    "label2id = {label:i for i, label in enumerate(tru_label) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the datasest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    DATA_PATH = Path(\"/content/drive/MyDrive/ner/data/swt\")# path for processed data from data preprocessing.ipynb\n",
    "    MODELS_BASE_PATH = Path(\"/content/drive/MyDrive/ner/models\")# path to save model\n",
    "else:\n",
    "    folder_path = Path(\"C:/Users/Gbadamosi/Documents/Nerd Corner/Master in ds and AI/MSC project/workspace/workspace/ner/ner/data/swt/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_file(folder_path, doc): \n",
    "    file_path = doc + '.json'\n",
    "    file_name = os.path.join(folder_path, file_path )\n",
    "\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\"base_abstract_with_punc_base_2023_12_03\", \"base_abstract_meth_with_punc_2023_12_03\", \"base_50_abstract_meth_with_punc_2023_12_03\", \"base_512_abstract_meth_with_punc_2023_12_03\"]\n",
    "abstract= read_file(DATA_PATH, file_names[0])\n",
    "abstract_methods = read_file(DATA_PATH, file_names[1])\n",
    "splitted_50_abstract_methods_context = read_file(DATA_PATH, file_names[2])\n",
    "splitted_512_records_method_context =read_file(DATA_PATH, file_names[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pmid': '16960863',\n",
       " 'paper': 'Burn2006_PMID16960863_1.pdf',\n",
       " 'text': \"Effects of rivastigmine in patients with and without visual hallucinations in dementia associated with Parkinson’s disease.  We aimed to determine prospectively whether rivastigmine, an inhibitor of acetylcholinesterase and butyrylcholinesterase, provided benefits in patients with and without visual hallucinations in a population with dementia associated with Parkinson's disease (PDD). This was a 24-week double-blind placebo-controlled study. Primary efficacy measures were the Alzheimer's Disease Assessment Scale cognitive subscale (ADAS-cog) and Alzheimer's Disease Cooperative Study-Clinician's Global Impression of Change (ADCS-CGIC). Secondary efficacy measures included activities of daily living, behavioral symptoms, and executive and attentional functions. Patients were stratified according to the presence of visual hallucinations at baseline. The study included 188 visual hallucinators (118 on rivastigmine, 70 on placebo) and 348 nonvisual hallucinators (239 on rivastigmine, 109 on placebo). Rivastigmine provided benefits in both visual hallucinators and nonvisual hallucinators. Absolute responses to rivastigmine on the ADAS-cog were comparable over 6 months, although rivastigmine-placebo differences tended to be larger in visual hallucinators (4.27; P = 0.002) than in nonhallucinators (2.09; P = 0.015). On the ADCS-CGIC, differences between rivastigmine and placebo were 0.5 in visual hallucinators (P = 0.030) and 0.3 in nonhallucinators (P = 0.111). Rivastigmine provided benefits on all secondary efficacy measures, and placebo declines and treatment differences were more marked in visual hallucinators. Adverse events were reported more frequently by rivastigmine-treated patients, although this difference was less marked in visual hallucinators. Visual hallucinations appear to predict more rapid decline and possibly greater therapeutic benefit from rivastigmine treatment in PDD.\\n\\nStudy Design The methodology of the double-blind study has been described previously.11 Patients had a diagnosis of PD according to the Queen’s Square Brain Bank clinical diagnostic criteria12 and dementia due to PD according to the fourth edition of the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV; code 294.1).13 Patients had mild to moderately severe dementia asdefined by a Mini-Mental State Examination (MMSE) score of 10 to 24. Patients were randomized to rivastigmine or placebo in a 2:1 ratio, which permitted the collection of more safety data in the rivastigmine group. The study included a 16-week dose escalation phase in order to reach maximum tolerated doses of rivastigmine up to 12 mg/day, which was maintained for another 8 weeks. At baseline and at weeks 16 and 24,efficacymeasures were performed. Primary measures were the cognitive subscale of the Alzheimer’s Disease Assessment Scale (ADAS-cog)14 and the Alzheimer’s Disease Cooperative Study-Clinician’s Global Impression of Change (ADCSCGIC).15 Secondary efficacy parameterswere theAlzheimer’s Disease Cooperative Study-Activities of Daily Living (ADCS-ADL),16 verbal fluency tests from the Delis-Kaplan Executive Function System (D-KEFS),17 Power of Attention [including Choice Reaction Time (CRT)] from the Cognitive Drug Research (CDR) attention battery,18 the MMSE,19 and the 10-item Neuropsychiatric Inventory (NPI-10).20 Safety evaluations included recording adverse events, laboratory parameters, and ECG. Extrapyramidal signs were assessed with the motor subsection of the Unified Parkinson’sDisease Rating Scale (UPDRS, part III).21 The protocol, informed consent form, and to patients and caregivers were reviewed by tutional review boards. All procedures w dance with the Helsinki Declaration as rev information local instiere in accorised in 1983. Analyses of Response in Visual Hallucinators vs. Nonhallucinators This was a prospective analysis that was described in the protocol. The presence or absence of any hallucinaariables  nd conefficacy  ded last for the ITT + observed case tions at baseline was recorded using the NPI-10. Caregivers were asked to tick an extra box marking the presence of visual hallucinations. Two subgroups were defined: those with visual hallucinations and those without visual hallucinations (but who might have other kinds of hallucination) at baseline. All patients who received at least one dose of study medication and had at least one safety measurement after baseline were included in safety analyses. Mainefficacy analyses were based on an intention-to-treat and retrieved dropout (ITT + RDO) population, defined as all randomized patients who received at least one dose of study medication and had a least one assessment on study drug for one of the primary efficacyv (ITT) and the patients who discontinued early a tinued to attend original scheduled visits for evaluations (RDO). Supportive analyses inclu observation carried forward (LOCF; as RDO but with an LOCF imputation) and (OC; no imputation) populations. Changes from baseline on the ADAS-cog, ADL, CDR, and NPI-10 were analyzed using an a of covariance (ANCOVA) model using treatme country as factors and baseline scores as cov ADCS-CGIC, MMSE, and D-KEFS scores, as changes on individual items on the NPI, were a using the van Elteren test, controlling for country the CDR, a composite Power of Attention sco defined that reflected, in everydayterms, concentration.” Percentages of patients changing (increasing or decreasing dosage, or initiating use) dopaminergic medication use over the course of the study were recorded. To compare medications with different potencies in the dopamine agonist category, the daily dose of each one was converted into the equivalent pramipexole dosage based on midpoints of recommended average dosages reported in the literature. For example, 2.75 mg/day of pergolide (range, 2-3.5 mg/day) was considered equivalent to 3 mg/day of pramipexole (range, 1.5- 4.5 mg/day), which resulted in a conversion factor of 3/2.75. Thus, if a patient received 5.5 mg/day of pergolide, it was considered to be the equivalent of taking 6 mg/day of pramipexole. This is considered a valid approach for comparing dosages of medications with different potencies. ADCSnalysis  nt and ariates. well as nalyzed . Using re was “effortful\",\n",
       " 'context': {'design_2': \"Parkinson's disease (PDD). This was a 24-week double-blind placebo-controlled study. Primary efficacy measures were the\",\n",
       "  'N_A': 'study included 188 visual hallucinators (118 on rivastigmine, 70 on placebo)',\n",
       "  'N_B': '188 visual hallucinators (118 on rivastigmine, 70 on placebo) and 348 nonvisual',\n",
       "  'N_C': 'placebo) and 348 nonvisual hallucinators (239 on rivastigmine, 109 on placebo).',\n",
       "  'N_D': '348 nonvisual hallucinators (239 on rivastigmine, 109 on placebo). Rivastigmine provided benefits',\n",
       "  'subjects': 'acetylcholinesterase and butyrylcholinesterase, provided benefits in patients with and without visual hallucinations in a population with dementia'},\n",
       " 'annotations': {'Sticky Note': '',\n",
       "  'condition': 'dementia associated with Parkinson’s disease (PDD)',\n",
       "  'design_1': 'randomized',\n",
       "  'design_2': '24-week double-blind placebo-controlled study',\n",
       "  'group_A_1': 'Visual hallucinators',\n",
       "  'group_A_2': 'Rivastigmine',\n",
       "  'group_B_1': 'visual hallucinators',\n",
       "  'group_B_2': 'Placebo',\n",
       "  'group_C_1': 'Nonhallucinators',\n",
       "  'group_C_2': 'Rivastigmine',\n",
       "  'group_D_1': 'nonhallucinators',\n",
       "  'group_D_2': 'Placebo',\n",
       "  'N_A': '118',\n",
       "  'N_B': '70',\n",
       "  'N_C': '239',\n",
       "  'N_D': '109',\n",
       "  'subjects': 'patients with and without visual hallucinations'}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_methods[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tokenize_align_labels(example, label_all_tokens = True):\n",
    "    # tokenized_input = tokenizer(example['tokens'], truncation = True, is_split_into_words=True, max_length = 450)\n",
    "    tokenized_input = tokenizer(example['tokens'], truncation = True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i , label in enumerate(example['ner_tags']):\n",
    "        word_ids = tokenized_input.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None: \n",
    "                label_ids.append(-100)\n",
    "            elif word_ids!= previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else: \n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100) \n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_input['labels'] = labels\n",
    "    return tokenized_input\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at kamalkraj/BioELECTRA-PICO and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([4, 768]) in the checkpoint and torch.Size([15, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([15]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"kamalkraj/BioELECTRA-PICO\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, model_max_length = 512)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, ignore_mismatched_sizes=True, id2label = id2label, label2id = label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = evaluate.load('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from functools import partial \n",
    "\n",
    "from preprocessing import *\n",
    "\n",
    "def token_f1_cond(true, pred, labels):\n",
    "    class_scores = zip(labels, precision_score(true, pred, labels=labels, average=None, zero_division=True),\n",
    "                       recall_score(true, pred, labels=labels, average=None))\n",
    "    result = {label: {\"f1\": get_f1(prec, rec), \"p\": prec, \"r\": rec} for label, prec, rec in class_scores}\n",
    "    return result\n",
    "def get_f1(prec, rec):\n",
    "    return 2 * prec * rec / (prec + rec)\n",
    "\n",
    "def compute_metrics_by_token_swt(p: Tuple[list, list], label_list):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [label_list[p] for prediction, label in zip(predictions, labels) for (p, l) in\n",
    "                        zip(prediction, label) if l != -100]\n",
    "    true_labels = [label_list[l] for prediction, label in zip(predictions, labels) for (p, l) in zip(prediction, label)\n",
    "                   if l != -100]\n",
    "\n",
    "    metrics_report = token_f1_cond(true_labels, true_predictions, label_list)\n",
    "    metrics_report_f1 = {\"f1_\" + k: v[\"f1\"] for k, v in metrics_report.items() if k != \"O\"}\n",
    "    return metrics_report_f1\n",
    "compute_metrics_by_token_swt = partial(compute_metrics_by_token_swt, label_list=tru_label) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs: 4, model_max_length: 512, gradient_acc_steps: 4, n_epochs: 5, lr: 5e-05\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_max_length = 512\n",
    "\n",
    "if COLAB:\n",
    "    batch_size = 16 # for p-100 16 is ok. For T4: 12\n",
    "    if model_max_length <= 384:\n",
    "      # at least 15 GB gpu\n",
    "        batch_size = 16\n",
    "    gradient_accumulation_steps = 1\n",
    "    num_train_epochs = 8\n",
    "else:\n",
    "    batch_size = 4\n",
    "    if model_max_length==256:\n",
    "        batch_size = 9\n",
    "\n",
    "    gradient_accumulation_steps = 4\n",
    "    num_train_epochs = 5\n",
    "\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 0.003 \n",
    "\n",
    "print(f\"bs: {batch_size}, model_max_length: {model_max_length}, gradient_acc_steps: {gradient_accumulation_steps}, \\\n",
    "n_epochs: {num_train_epochs}, lr: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABSTRACT ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Huggingface Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 1520/1520 [00:00<00:00, 25838.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hf_ds = create_datasets(abstract, tru_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pmid': Value(dtype='string', id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-cond', 'I-cond', 'B-des', 'I-des', 'B-subj', 'I-subj', 'B-group_A', 'I-group_A', 'B-group_B', 'I-group_B', 'B-group_C', 'I-group_C', 'B-group_D', 'I-group_D'], id=None), length=-1, id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pmid': '16960863',\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  11,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  9,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  11,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  9,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'tokens': ['effects',\n",
       "  'of',\n",
       "  'rivastigmine',\n",
       "  'in',\n",
       "  'patients',\n",
       "  'with',\n",
       "  'and',\n",
       "  'without',\n",
       "  'visual',\n",
       "  'hallucinations',\n",
       "  'in',\n",
       "  'dementia',\n",
       "  'associated',\n",
       "  'with',\n",
       "  'parkinsons',\n",
       "  'disease',\n",
       "  'we',\n",
       "  'aimed',\n",
       "  'to',\n",
       "  'determine',\n",
       "  'prospectively',\n",
       "  'whether',\n",
       "  'rivastigmine',\n",
       "  'an',\n",
       "  'inhibitor',\n",
       "  'of',\n",
       "  'acetylcholinesterase',\n",
       "  'and',\n",
       "  'butyrylcholinesterase',\n",
       "  'provided',\n",
       "  'benefits',\n",
       "  'in',\n",
       "  'patients',\n",
       "  'with',\n",
       "  'and',\n",
       "  'without',\n",
       "  'visual',\n",
       "  'hallucinations',\n",
       "  'in',\n",
       "  'a',\n",
       "  'population',\n",
       "  'with',\n",
       "  'dementia',\n",
       "  'associated',\n",
       "  'with',\n",
       "  'parkinsons',\n",
       "  'disease',\n",
       "  'this',\n",
       "  'was',\n",
       "  'a',\n",
       "  '24-week',\n",
       "  'double-blind',\n",
       "  'placebo-controlled',\n",
       "  'study',\n",
       "  'primary',\n",
       "  'efficacy',\n",
       "  'measures',\n",
       "  'were',\n",
       "  'the',\n",
       "  'alzheimers',\n",
       "  'disease',\n",
       "  'assessment',\n",
       "  'scale',\n",
       "  'cognitive',\n",
       "  'subscale',\n",
       "  '(adas-cog)',\n",
       "  'and',\n",
       "  'alzheimers',\n",
       "  'disease',\n",
       "  'cooperative',\n",
       "  'study-clinicians',\n",
       "  'global',\n",
       "  'impression',\n",
       "  'of',\n",
       "  'change',\n",
       "  '(adcs-cgic)',\n",
       "  'secondary',\n",
       "  'efficacy',\n",
       "  'measures',\n",
       "  'included',\n",
       "  'activities',\n",
       "  'of',\n",
       "  'daily',\n",
       "  'living',\n",
       "  'behavioral',\n",
       "  'symptoms',\n",
       "  'and',\n",
       "  'executive',\n",
       "  'and',\n",
       "  'attentional',\n",
       "  'functions',\n",
       "  'patients',\n",
       "  'were',\n",
       "  'stratified',\n",
       "  'according',\n",
       "  'to',\n",
       "  'the',\n",
       "  'presence',\n",
       "  'of',\n",
       "  'visual',\n",
       "  'hallucinations',\n",
       "  'at',\n",
       "  'baseline',\n",
       "  'the',\n",
       "  'study',\n",
       "  'included',\n",
       "  '188',\n",
       "  'visual',\n",
       "  'hallucinators',\n",
       "  '(118',\n",
       "  'on',\n",
       "  'rivastigmine',\n",
       "  '70',\n",
       "  'on',\n",
       "  'placebo)',\n",
       "  'and',\n",
       "  '348',\n",
       "  'nonvisual',\n",
       "  'hallucinators',\n",
       "  '(239',\n",
       "  'on',\n",
       "  'rivastigmine',\n",
       "  '109',\n",
       "  'on',\n",
       "  'placebo)',\n",
       "  'rivastigmine',\n",
       "  'provided',\n",
       "  'benefits',\n",
       "  'in',\n",
       "  'both',\n",
       "  'visual',\n",
       "  'hallucinators',\n",
       "  'and',\n",
       "  'nonvisual',\n",
       "  'hallucinators',\n",
       "  'absolute',\n",
       "  'responses',\n",
       "  'to',\n",
       "  'rivastigmine',\n",
       "  'on',\n",
       "  'the',\n",
       "  'adas-cog',\n",
       "  'were',\n",
       "  'comparable',\n",
       "  'over',\n",
       "  '6',\n",
       "  'months',\n",
       "  'although',\n",
       "  'rivastigmine-placebo',\n",
       "  'differences',\n",
       "  'tended',\n",
       "  'to',\n",
       "  'be',\n",
       "  'larger',\n",
       "  'in',\n",
       "  'visual',\n",
       "  'hallucinators',\n",
       "  '(4',\n",
       "  '27',\n",
       "  'p',\n",
       "  '0',\n",
       "  '002)',\n",
       "  'than',\n",
       "  'in',\n",
       "  'nonhallucinators',\n",
       "  '(2',\n",
       "  '09',\n",
       "  'p',\n",
       "  '0',\n",
       "  '015)',\n",
       "  'on',\n",
       "  'the',\n",
       "  'adcs-cgic',\n",
       "  'differences',\n",
       "  'between',\n",
       "  'rivastigmine',\n",
       "  'and',\n",
       "  'placebo',\n",
       "  'were',\n",
       "  '0',\n",
       "  '5',\n",
       "  'in',\n",
       "  'visual',\n",
       "  'hallucinators',\n",
       "  '(p',\n",
       "  '0',\n",
       "  '030)',\n",
       "  'and',\n",
       "  '0',\n",
       "  '3',\n",
       "  'in',\n",
       "  'nonhallucinators',\n",
       "  '(p',\n",
       "  '0',\n",
       "  '111)',\n",
       "  'rivastigmine',\n",
       "  'provided',\n",
       "  'benefits',\n",
       "  'on',\n",
       "  'all',\n",
       "  'secondary',\n",
       "  'efficacy',\n",
       "  'measures',\n",
       "  'and',\n",
       "  'placebo',\n",
       "  'declines',\n",
       "  'and',\n",
       "  'treatment',\n",
       "  'differences',\n",
       "  'were',\n",
       "  'more',\n",
       "  'marked',\n",
       "  'in',\n",
       "  'visual',\n",
       "  'hallucinators',\n",
       "  'adverse',\n",
       "  'events',\n",
       "  'were',\n",
       "  'reported',\n",
       "  'more',\n",
       "  'frequently',\n",
       "  'by',\n",
       "  'rivastigmine-treated',\n",
       "  'patients',\n",
       "  'although',\n",
       "  'this',\n",
       "  'difference',\n",
       "  'was',\n",
       "  'less',\n",
       "  'marked',\n",
       "  'in',\n",
       "  'visual',\n",
       "  'hallucinators',\n",
       "  'visual',\n",
       "  'hallucinations',\n",
       "  'appear',\n",
       "  'to',\n",
       "  'predict',\n",
       "  'more',\n",
       "  'rapid',\n",
       "  'decline',\n",
       "  'and',\n",
       "  'possibly',\n",
       "  'greater',\n",
       "  'therapeutic',\n",
       "  'benefit',\n",
       "  'from',\n",
       "  'rivastigmine',\n",
       "  'treatment',\n",
       "  'in',\n",
       "  'pdd']}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_ds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_train_test = train_test_split(hf_ds, train_test_size=0.2, validation_size=0.5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pmid', 'ner_tags', 'tokens'],\n",
       "        num_rows: 1394\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pmid', 'ner_tags', 'tokens'],\n",
       "        num_rows: 78\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['pmid', 'ner_tags', 'tokens'],\n",
       "        num_rows: 77\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-cond',\n",
       " 'I-cond',\n",
       " 'B-des',\n",
       " 'I-des',\n",
       " 'B-subj',\n",
       " 'I-subj',\n",
       " 'B-group_A',\n",
       " 'I-group_A',\n",
       " 'B-group_B',\n",
       " 'I-group_B',\n",
       " 'B-group_C',\n",
       " 'I-group_C',\n",
       " 'B-group_D',\n",
       " 'I-group_D']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = part_train_test['train'].features['ner_tags'].feature.names\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer Example and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['efficacy',\n",
       " 'of',\n",
       " 'metformin',\n",
       " 'versus',\n",
       " 'insulin',\n",
       " 'in',\n",
       " 'the',\n",
       " 'management',\n",
       " 'of',\n",
       " 'pregnancy',\n",
       " 'with',\n",
       " 'diabetes',\n",
       " 'objective',\n",
       " 'to',\n",
       " 'compare',\n",
       " 'the',\n",
       " 'efficacy',\n",
       " 'of',\n",
       " 'metformin',\n",
       " 'with',\n",
       " 'insulin',\n",
       " 'in',\n",
       " 'the',\n",
       " 'management',\n",
       " 'of',\n",
       " 'pregnancy',\n",
       " 'with',\n",
       " 'diabetes',\n",
       " 'study',\n",
       " 'design',\n",
       " 'randomized',\n",
       " 'clinical',\n",
       " 'trial',\n",
       " 'place',\n",
       " 'and',\n",
       " 'duration',\n",
       " 'of',\n",
       " 'study',\n",
       " 'department',\n",
       " 'of',\n",
       " 'obstetrics',\n",
       " 'and',\n",
       " 'gynaecology',\n",
       " 'maternal',\n",
       " 'and',\n",
       " 'child',\n",
       " 'health',\n",
       " 'centre',\n",
       " 'pakistan',\n",
       " 'institute',\n",
       " 'of',\n",
       " 'medical',\n",
       " 'sciences',\n",
       " 'islamabad',\n",
       " 'from',\n",
       " 'may',\n",
       " '2010',\n",
       " 'to',\n",
       " 'january',\n",
       " '2011',\n",
       " 'methodology',\n",
       " 'a',\n",
       " 'total',\n",
       " 'of',\n",
       " '68',\n",
       " 'pregnant',\n",
       " 'patients',\n",
       " 'with',\n",
       " 'diabetes',\n",
       " 'were',\n",
       " 'included',\n",
       " 'in',\n",
       " 'this',\n",
       " 'study',\n",
       " 'patients',\n",
       " 'were',\n",
       " 'randomly',\n",
       " 'divided',\n",
       " 'in',\n",
       " 'to',\n",
       " 'two',\n",
       " 'groups',\n",
       " 'of',\n",
       " 'each',\n",
       " '34',\n",
       " 'patients',\n",
       " 'based',\n",
       " 'on',\n",
       " 'table',\n",
       " 'of',\n",
       " 'random',\n",
       " 'numbers',\n",
       " 'one',\n",
       " 'was',\n",
       " 'labelled',\n",
       " 'as',\n",
       " 'group-a',\n",
       " 'and',\n",
       " 'other',\n",
       " 'was',\n",
       " 'labelled',\n",
       " 'as',\n",
       " 'group-b',\n",
       " 'group-a',\n",
       " 'received',\n",
       " 'insulin',\n",
       " 'and',\n",
       " 'group-b',\n",
       " 'received',\n",
       " 'metformin',\n",
       " 'for',\n",
       " 'the',\n",
       " 'management',\n",
       " 'of',\n",
       " 'diabetes',\n",
       " 'results',\n",
       " 'the',\n",
       " 'mean',\n",
       " 'age',\n",
       " 'was',\n",
       " '29',\n",
       " '82',\n",
       " '-',\n",
       " '4',\n",
       " '58',\n",
       " 'and',\n",
       " '29',\n",
       " '35',\n",
       " '-',\n",
       " '4',\n",
       " '97',\n",
       " 'years',\n",
       " 'in',\n",
       " 'groups-a',\n",
       " 'and',\n",
       " 'b',\n",
       " 'respectively',\n",
       " 'fasting',\n",
       " 'blood',\n",
       " 'sugar',\n",
       " 'level',\n",
       " 'after',\n",
       " '1',\n",
       " 'month',\n",
       " 'was',\n",
       " 'controlled',\n",
       " 'in',\n",
       " '22',\n",
       " '(64',\n",
       " '7%)',\n",
       " 'patients',\n",
       " 'in',\n",
       " 'group-a',\n",
       " 'and',\n",
       " 'in',\n",
       " '27',\n",
       " '(79',\n",
       " '4%)',\n",
       " 'in',\n",
       " 'group-b',\n",
       " '(p',\n",
       " '0',\n",
       " '05)',\n",
       " 'fasting',\n",
       " 'blood',\n",
       " 'sugar',\n",
       " 'level',\n",
       " 'at',\n",
       " 'term',\n",
       " 'remained',\n",
       " 'controlled',\n",
       " 'in',\n",
       " '30',\n",
       " '(88',\n",
       " '2%)',\n",
       " 'patients',\n",
       " 'in',\n",
       " 'group-a',\n",
       " 'and',\n",
       " '27',\n",
       " '(79',\n",
       " '4%)',\n",
       " 'in',\n",
       " 'group-b',\n",
       " '(p',\n",
       " '0',\n",
       " '05)',\n",
       " 'comparison',\n",
       " 'of',\n",
       " 'random',\n",
       " 'blood',\n",
       " 'sugar',\n",
       " 'levels',\n",
       " 'within',\n",
       " 'normal',\n",
       " 'limits',\n",
       " 'after',\n",
       " '1',\n",
       " 'month',\n",
       " 'in',\n",
       " '25',\n",
       " '(73',\n",
       " '5%)',\n",
       " 'in',\n",
       " 'group-a',\n",
       " 'and',\n",
       " 'in',\n",
       " '24',\n",
       " '(70',\n",
       " '6%)',\n",
       " 'in',\n",
       " 'group-b',\n",
       " 'at',\n",
       " 'term',\n",
       " 'random',\n",
       " 'blood',\n",
       " 'sugar',\n",
       " 'level',\n",
       " 'was',\n",
       " 'controlled',\n",
       " 'in',\n",
       " '28',\n",
       " '(82',\n",
       " '4%)',\n",
       " 'and',\n",
       " '27',\n",
       " '(79',\n",
       " '4%)',\n",
       " 'patients',\n",
       " 'in',\n",
       " 'group-a',\n",
       " 'and',\n",
       " 'b',\n",
       " 'respectively',\n",
       " 'comparison',\n",
       " 'of',\n",
       " 'post-treatment',\n",
       " 'hba1c',\n",
       " 'level',\n",
       " 'depicts',\n",
       " 'that',\n",
       " 'diabetes',\n",
       " 'controlled',\n",
       " 'in',\n",
       " '27',\n",
       " '(79',\n",
       " '4%)',\n",
       " 'patients',\n",
       " 'in',\n",
       " 'group-a',\n",
       " 'while',\n",
       " 'in',\n",
       " '28',\n",
       " '(82',\n",
       " '3%)',\n",
       " 'patients',\n",
       " 'of',\n",
       " 'group-b',\n",
       " 'the',\n",
       " 'efficacy',\n",
       " 'of',\n",
       " 'metformin',\n",
       " 'and',\n",
       " 'insulin',\n",
       " 'in',\n",
       " 'controlling',\n",
       " 'diabetes',\n",
       " 'was',\n",
       " 'equal',\n",
       " 'in',\n",
       " 'two',\n",
       " 'groups',\n",
       " 'conclusion',\n",
       " 'there',\n",
       " 'was',\n",
       " 'no',\n",
       " 'marked',\n",
       " 'difference',\n",
       " 'in',\n",
       " 'efficacy',\n",
       " 'of',\n",
       " 'metformin',\n",
       " 'and',\n",
       " 'insulin',\n",
       " 'in',\n",
       " 'controlling',\n",
       " 'diabetes',\n",
       " 'in',\n",
       " 'pregnant',\n",
       " 'patients',\n",
       " 'in',\n",
       " 'two',\n",
       " 'groups']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = part_train_test['train'][0]\n",
    "example_text['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 53,\n",
       " 53,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 96,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 148,\n",
       " 149,\n",
       " 149,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 152,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 156,\n",
       " 157,\n",
       " 157,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 159,\n",
       " 159,\n",
       " 160,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 173,\n",
       " 174,\n",
       " 174,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 177,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 180,\n",
       " 181,\n",
       " 181,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 183,\n",
       " 183,\n",
       " 184,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 201,\n",
       " 202,\n",
       " 202,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 204,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 208,\n",
       " 209,\n",
       " 209,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 211,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 222,\n",
       " 223,\n",
       " 223,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 226,\n",
       " 227,\n",
       " 227,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 230,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 236,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 245,\n",
       " 246,\n",
       " 246,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 249,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 253,\n",
       " 254,\n",
       " 254,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 257,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " None]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenized_input = tokenizer(example_text['tokens'], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'])\n",
    "\n",
    "words_ids = tokenized_input.word_ids()\n",
    "words_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'efficacy',\n",
       " 'of',\n",
       " 'metformin',\n",
       " 'versus',\n",
       " 'insulin',\n",
       " 'in',\n",
       " 'the',\n",
       " 'management',\n",
       " 'of',\n",
       " 'pregnancy',\n",
       " 'with',\n",
       " 'diabetes',\n",
       " 'objective',\n",
       " 'to',\n",
       " 'compare',\n",
       " 'the',\n",
       " 'efficacy',\n",
       " 'of',\n",
       " 'metformin',\n",
       " 'with',\n",
       " 'insulin',\n",
       " 'in',\n",
       " 'the',\n",
       " 'management',\n",
       " 'of',\n",
       " 'pregnancy',\n",
       " 'with',\n",
       " 'diabetes',\n",
       " 'study',\n",
       " 'design',\n",
       " 'randomized',\n",
       " 'clinical',\n",
       " 'trial',\n",
       " 'place',\n",
       " 'and',\n",
       " 'duration',\n",
       " 'of',\n",
       " 'study',\n",
       " 'department',\n",
       " 'of',\n",
       " 'obstetrics',\n",
       " 'and',\n",
       " 'gynaec',\n",
       " '##ology',\n",
       " 'maternal',\n",
       " 'and',\n",
       " 'child',\n",
       " 'health',\n",
       " 'centre',\n",
       " 'pakistan',\n",
       " 'institute',\n",
       " 'of',\n",
       " 'medical',\n",
       " 'sciences',\n",
       " 'isl',\n",
       " '##ama',\n",
       " '##ba',\n",
       " '##d',\n",
       " 'from',\n",
       " 'may',\n",
       " '2010',\n",
       " 'to',\n",
       " 'january',\n",
       " '2011',\n",
       " 'methodology',\n",
       " 'a',\n",
       " 'total',\n",
       " 'of',\n",
       " '68',\n",
       " 'pregnant',\n",
       " 'patients',\n",
       " 'with',\n",
       " 'diabetes',\n",
       " 'were',\n",
       " 'included',\n",
       " 'in',\n",
       " 'this',\n",
       " 'study',\n",
       " 'patients',\n",
       " 'were',\n",
       " 'randomly',\n",
       " 'divided',\n",
       " 'in',\n",
       " 'to',\n",
       " 'two',\n",
       " 'groups',\n",
       " 'of',\n",
       " 'each',\n",
       " '34',\n",
       " 'patients',\n",
       " 'based',\n",
       " 'on',\n",
       " 'table',\n",
       " 'of',\n",
       " 'random',\n",
       " 'numbers',\n",
       " 'one',\n",
       " 'was',\n",
       " 'labelled',\n",
       " 'as',\n",
       " 'group',\n",
       " '-',\n",
       " 'a',\n",
       " 'and',\n",
       " 'other',\n",
       " 'was',\n",
       " 'labelled',\n",
       " 'as',\n",
       " 'group',\n",
       " '-',\n",
       " 'b',\n",
       " 'group',\n",
       " '-',\n",
       " 'a',\n",
       " 'received',\n",
       " 'insulin',\n",
       " 'and',\n",
       " 'group',\n",
       " '-',\n",
       " 'b',\n",
       " 'received',\n",
       " 'metformin',\n",
       " 'for',\n",
       " 'the',\n",
       " 'management',\n",
       " 'of',\n",
       " 'diabetes',\n",
       " 'results',\n",
       " 'the',\n",
       " 'mean',\n",
       " 'age',\n",
       " 'was',\n",
       " '29',\n",
       " '82',\n",
       " '-',\n",
       " '4',\n",
       " '58',\n",
       " 'and',\n",
       " '29',\n",
       " '35',\n",
       " '-',\n",
       " '4',\n",
       " '97',\n",
       " 'years',\n",
       " 'in',\n",
       " 'groups',\n",
       " '-',\n",
       " 'a',\n",
       " 'and',\n",
       " 'b',\n",
       " 'respectively',\n",
       " 'fasting',\n",
       " 'blood',\n",
       " 'sugar',\n",
       " 'level',\n",
       " 'after',\n",
       " '1',\n",
       " 'month',\n",
       " 'was',\n",
       " 'controlled',\n",
       " 'in',\n",
       " '22',\n",
       " '(',\n",
       " '64',\n",
       " '7',\n",
       " '%',\n",
       " ')',\n",
       " 'patients',\n",
       " 'in',\n",
       " 'group',\n",
       " '-',\n",
       " 'a',\n",
       " 'and',\n",
       " 'in',\n",
       " '27',\n",
       " '(',\n",
       " '79',\n",
       " '4',\n",
       " '%',\n",
       " ')',\n",
       " 'in',\n",
       " 'group',\n",
       " '-',\n",
       " 'b',\n",
       " '(',\n",
       " 'p',\n",
       " '0',\n",
       " '05',\n",
       " ')',\n",
       " 'fasting',\n",
       " 'blood',\n",
       " 'sugar',\n",
       " 'level',\n",
       " 'at',\n",
       " 'term',\n",
       " 'remained',\n",
       " 'controlled',\n",
       " 'in',\n",
       " '30',\n",
       " '(',\n",
       " '88',\n",
       " '2',\n",
       " '%',\n",
       " ')',\n",
       " 'patients',\n",
       " 'in',\n",
       " 'group',\n",
       " '-',\n",
       " 'a',\n",
       " 'and',\n",
       " '27',\n",
       " '(',\n",
       " '79',\n",
       " '4',\n",
       " '%',\n",
       " ')',\n",
       " 'in',\n",
       " 'group',\n",
       " '-',\n",
       " 'b',\n",
       " '(',\n",
       " 'p',\n",
       " '0',\n",
       " '05',\n",
       " ')',\n",
       " 'comparison',\n",
       " 'of',\n",
       " 'random',\n",
       " 'blood',\n",
       " 'sugar',\n",
       " 'levels',\n",
       " 'within',\n",
       " 'normal',\n",
       " 'limits',\n",
       " 'after',\n",
       " '1',\n",
       " 'month',\n",
       " 'in',\n",
       " '25',\n",
       " '(',\n",
       " '73',\n",
       " '5',\n",
       " '%',\n",
       " ')',\n",
       " 'in',\n",
       " 'group',\n",
       " '-',\n",
       " 'a',\n",
       " 'and',\n",
       " 'in',\n",
       " '24',\n",
       " '(',\n",
       " '70',\n",
       " '6',\n",
       " '%',\n",
       " ')',\n",
       " 'in',\n",
       " 'group',\n",
       " '-',\n",
       " 'b',\n",
       " 'at',\n",
       " 'term',\n",
       " 'random',\n",
       " 'blood',\n",
       " 'sugar',\n",
       " 'level',\n",
       " 'was',\n",
       " 'controlled',\n",
       " 'in',\n",
       " '28',\n",
       " '(',\n",
       " '82',\n",
       " '4',\n",
       " '%',\n",
       " ')',\n",
       " 'and',\n",
       " '27',\n",
       " '(',\n",
       " '79',\n",
       " '4',\n",
       " '%',\n",
       " ')',\n",
       " 'patients',\n",
       " 'in',\n",
       " 'group',\n",
       " '-',\n",
       " 'a',\n",
       " 'and',\n",
       " 'b',\n",
       " 'respectively',\n",
       " 'comparison',\n",
       " 'of',\n",
       " 'post',\n",
       " '-',\n",
       " 'treatment',\n",
       " 'hba1c',\n",
       " 'level',\n",
       " 'depic',\n",
       " '##ts',\n",
       " 'that',\n",
       " 'diabetes',\n",
       " 'controlled',\n",
       " 'in',\n",
       " '27',\n",
       " '(',\n",
       " '79',\n",
       " '4',\n",
       " '%',\n",
       " ')',\n",
       " 'patients',\n",
       " 'in',\n",
       " 'group',\n",
       " '-',\n",
       " 'a',\n",
       " 'while',\n",
       " 'in',\n",
       " '28',\n",
       " '(',\n",
       " '82',\n",
       " '3',\n",
       " '%',\n",
       " ')',\n",
       " 'patients',\n",
       " 'of',\n",
       " 'group',\n",
       " '-',\n",
       " 'b',\n",
       " 'the',\n",
       " 'efficacy',\n",
       " 'of',\n",
       " 'metformin',\n",
       " 'and',\n",
       " 'insulin',\n",
       " 'in',\n",
       " 'controlling',\n",
       " 'diabetes',\n",
       " 'was',\n",
       " 'equal',\n",
       " 'in',\n",
       " 'two',\n",
       " 'groups',\n",
       " 'conclusion',\n",
       " 'there',\n",
       " 'was',\n",
       " 'no',\n",
       " 'marked',\n",
       " 'difference',\n",
       " 'in',\n",
       " 'efficacy',\n",
       " 'of',\n",
       " 'metformin',\n",
       " 'and',\n",
       " 'insulin',\n",
       " 'in',\n",
       " 'controlling',\n",
       " 'diabetes',\n",
       " 'in',\n",
       " 'pregnant',\n",
       " 'patients',\n",
       " 'in',\n",
       " 'two',\n",
       " 'groups',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293, 364)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_text['ner_tags']), len(tokenized_input['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[2, 3540, 1685, 12968, 3803, 3433, 1682, 1680, 3190, 1685, 4097, 1715, 3507, 2833, 1701, 4461, 1680, 3540, 1685, 12968, 1715, 3433, 1682, 1680, 3190, 1685, 4097, 1715, 3507, 1901, 2693, 4384, 2121, 4033, 3831, 1690, 3864, 1685, 1901, 6138, 1685, 19921, 1690, 19417, 2317, 4796, 1690, 2355, 2161, 8622, 22195, 9218, 1685, 3045, 13222, 28142, 14707, 7915, 1022, 1814, 2056, 6759, 1701, 6125, 7336, 6604, 42, 2453, 1685, 4980, 6157, 1808, 1715, 3507, 1748, 2990, 1682, 1805, 1901, 1808, 1748, 5585, 5130, 1682, 1701, 2033, 2345, 1685, 2562, 3930, 1808, 2234, 1755, 12685, 1685, 3077, 5352, 2155, 1734, 9802, 1732, 1922, 16, 42, 1690, 2187, 1734, 9802, 1732, 1922, 16, 43, 1922, 16, 42, 3484, 3433, 1690, 1922, 16, 43, 3484, 12968, 1725, 1680, 3190, 1685, 3507, 1890, 1680, 2433, 2293, 1734, 3718, 5283, 16, 23, 4853, 1690, 3718, 3460, 16, 23, 5375, 2281, 1682, 2345, 16, 42, 1690, 43, 2427, 7721, 2368, 8921, 1999, 1970, 20, 2444, 1734, 3843, 1682, 3184, 11, 4701, 26, 8, 12, 1808, 1682, 1922, 16, 42, 1690, 1682, 3615, 11, 5396, 23, 8, 12, 1682, 1922, 16, 43, 11, 57, 19, 2766, 12, 7721, 2368, 8921, 1999, 1798, 2384, 4688, 3843, 1682, 2618, 11, 5316, 21, 8, 12, 1808, 1682, 1922, 16, 42, 1690, 3615, 11, 5396, 23, 8, 12, 1682, 1922, 16, 43, 11, 57, 19, 2766, 12, 3801, 1685, 3077, 2368, 8921, 2182, 2520, 2488, 7015, 1970, 20, 2444, 1682, 2646, 11, 5048, 24, 8, 12, 1682, 1922, 16, 42, 1690, 1682, 2721, 11, 3824, 25, 8, 12, 1682, 1922, 16, 43, 1798, 2384, 3077, 2368, 8921, 1999, 1734, 3843, 1682, 3410, 11, 5283, 23, 8, 12, 1690, 3615, 11, 5396, 23, 8, 12, 1808, 1682, 1922, 16, 42, 1690, 43, 2427, 3801, 1685, 2373, 16, 2015, 11234, 1999, 25694, 1727, 1760, 3507, 3843, 1682, 3615, 11, 5396, 23, 8, 12, 1808, 1682, 1922, 16, 42, 2555, 1682, 3410, 11, 5283, 22, 8, 12, 1808, 1685, 1922, 16, 43, 1680, 3540, 1685, 12968, 1690, 3433, 1682, 6384, 3507, 1734, 6158, 1682, 2033, 2345, 2157, 2100, 1734, 1982, 4096, 3140, 1682, 3540, 1685, 12968, 1690, 3433, 1682, 6384, 3507, 1682, 6157, 1808, 1682, 2033, 2345, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 9, 0, 7, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 0, 7, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 7, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 7, 0, 0, 1, 0, 0, 0, 0, 0, 0, -100]]}\n"
     ]
    }
   ],
   "source": [
    "q = tokenize_align_labels(part_train_test['train'][0:1])\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]___________________________________ -100\n",
      "efficacy________________________________ 0\n",
      "of______________________________________ 0\n",
      "metformin_______________________________ 9\n",
      "versus__________________________________ 0\n",
      "insulin_________________________________ 7\n",
      "in______________________________________ 0\n",
      "the_____________________________________ 0\n",
      "management______________________________ 0\n",
      "of______________________________________ 0\n",
      "pregnancy_______________________________ 0\n",
      "with____________________________________ 0\n",
      "diabetes________________________________ 1\n",
      "objective_______________________________ 0\n",
      "to______________________________________ 0\n",
      "compare_________________________________ 0\n",
      "the_____________________________________ 0\n",
      "efficacy________________________________ 0\n",
      "of______________________________________ 0\n",
      "metformin_______________________________ 9\n",
      "with____________________________________ 0\n",
      "insulin_________________________________ 7\n",
      "in______________________________________ 0\n",
      "the_____________________________________ 0\n",
      "management______________________________ 0\n",
      "of______________________________________ 0\n",
      "pregnancy_______________________________ 0\n",
      "with____________________________________ 0\n",
      "diabetes________________________________ 1\n",
      "study___________________________________ 0\n",
      "design__________________________________ 0\n",
      "randomized______________________________ 3\n",
      "clinical________________________________ 4\n",
      "trial___________________________________ 4\n",
      "place___________________________________ 0\n",
      "and_____________________________________ 0\n",
      "duration________________________________ 0\n",
      "of______________________________________ 0\n",
      "study___________________________________ 0\n",
      "department______________________________ 0\n",
      "of______________________________________ 0\n",
      "obstetrics______________________________ 0\n",
      "and_____________________________________ 0\n",
      "gynaec__________________________________ 0\n",
      "##ology_________________________________ 0\n",
      "maternal________________________________ 0\n",
      "and_____________________________________ 0\n",
      "child___________________________________ 0\n",
      "health__________________________________ 0\n",
      "centre__________________________________ 0\n",
      "pakistan________________________________ 0\n",
      "institute_______________________________ 0\n",
      "of______________________________________ 0\n",
      "medical_________________________________ 0\n",
      "sciences________________________________ 0\n",
      "isl_____________________________________ 0\n",
      "##ama___________________________________ 0\n",
      "##ba____________________________________ 0\n",
      "##d_____________________________________ 0\n",
      "from____________________________________ 0\n",
      "may_____________________________________ 0\n",
      "2010____________________________________ 0\n",
      "to______________________________________ 0\n",
      "january_________________________________ 0\n",
      "2011____________________________________ 0\n",
      "methodology_____________________________ 0\n",
      "a_______________________________________ 0\n",
      "total___________________________________ 0\n",
      "of______________________________________ 0\n",
      "68______________________________________ 0\n",
      "pregnant________________________________ 0\n",
      "patients________________________________ 0\n",
      "with____________________________________ 0\n",
      "diabetes________________________________ 1\n",
      "were____________________________________ 0\n",
      "included________________________________ 0\n",
      "in______________________________________ 0\n",
      "this____________________________________ 0\n",
      "study___________________________________ 0\n",
      "patients________________________________ 0\n",
      "were____________________________________ 0\n",
      "randomly________________________________ 0\n",
      "divided_________________________________ 0\n",
      "in______________________________________ 0\n",
      "to______________________________________ 0\n",
      "two_____________________________________ 0\n",
      "groups__________________________________ 0\n",
      "of______________________________________ 0\n",
      "each____________________________________ 0\n",
      "34______________________________________ 0\n",
      "patients________________________________ 0\n",
      "based___________________________________ 0\n",
      "on______________________________________ 0\n",
      "table___________________________________ 0\n",
      "of______________________________________ 0\n",
      "random__________________________________ 0\n",
      "numbers_________________________________ 0\n",
      "one_____________________________________ 0\n",
      "was_____________________________________ 0\n",
      "labelled________________________________ 0\n",
      "as______________________________________ 0\n",
      "group___________________________________ 0\n",
      "-_______________________________________ 0\n",
      "a_______________________________________ 0\n",
      "and_____________________________________ 0\n",
      "other___________________________________ 0\n",
      "was_____________________________________ 0\n",
      "labelled________________________________ 0\n",
      "as______________________________________ 0\n",
      "group___________________________________ 0\n",
      "-_______________________________________ 0\n",
      "b_______________________________________ 0\n",
      "group___________________________________ 0\n",
      "-_______________________________________ 0\n",
      "a_______________________________________ 0\n",
      "received________________________________ 0\n",
      "insulin_________________________________ 7\n",
      "and_____________________________________ 0\n",
      "group___________________________________ 0\n",
      "-_______________________________________ 0\n",
      "b_______________________________________ 0\n",
      "received________________________________ 0\n",
      "metformin_______________________________ 9\n",
      "for_____________________________________ 0\n",
      "the_____________________________________ 0\n",
      "management______________________________ 0\n",
      "of______________________________________ 0\n",
      "diabetes________________________________ 1\n",
      "results_________________________________ 0\n",
      "the_____________________________________ 0\n",
      "mean____________________________________ 0\n",
      "age_____________________________________ 0\n",
      "was_____________________________________ 0\n",
      "29______________________________________ 0\n",
      "82______________________________________ 0\n",
      "-_______________________________________ 0\n",
      "4_______________________________________ 0\n",
      "58______________________________________ 0\n",
      "and_____________________________________ 0\n",
      "29______________________________________ 0\n",
      "35______________________________________ 0\n",
      "-_______________________________________ 0\n",
      "4_______________________________________ 0\n",
      "97______________________________________ 0\n",
      "years___________________________________ 0\n",
      "in______________________________________ 0\n",
      "groups__________________________________ 0\n",
      "-_______________________________________ 0\n",
      "a_______________________________________ 0\n",
      "and_____________________________________ 0\n",
      "b_______________________________________ 0\n",
      "respectively____________________________ 0\n",
      "fasting_________________________________ 0\n",
      "blood___________________________________ 0\n",
      "sugar___________________________________ 0\n",
      "level___________________________________ 0\n",
      "after___________________________________ 0\n",
      "1_______________________________________ 0\n",
      "month___________________________________ 0\n",
      "was_____________________________________ 0\n",
      "controlled______________________________ 0\n",
      "in______________________________________ 0\n",
      "22______________________________________ 0\n",
      "(_______________________________________ 0\n",
      "64______________________________________ 0\n",
      "7_______________________________________ 0\n",
      "%_______________________________________ 0\n",
      ")_______________________________________ 0\n",
      "patients________________________________ 0\n",
      "in______________________________________ 0\n",
      "group___________________________________ 0\n",
      "-_______________________________________ 0\n",
      "a_______________________________________ 0\n",
      "and_____________________________________ 0\n",
      "in______________________________________ 0\n",
      "27______________________________________ 0\n",
      "(_______________________________________ 0\n",
      "79______________________________________ 0\n",
      "4_______________________________________ 0\n",
      "%_______________________________________ 0\n",
      ")_______________________________________ 0\n",
      "in______________________________________ 0\n",
      "group___________________________________ 0\n",
      "-_______________________________________ 0\n",
      "b_______________________________________ 0\n",
      "(_______________________________________ 0\n",
      "p_______________________________________ 0\n",
      "0_______________________________________ 0\n",
      "05______________________________________ 0\n",
      ")_______________________________________ 0\n",
      "fasting_________________________________ 0\n",
      "blood___________________________________ 0\n",
      "sugar___________________________________ 0\n",
      "level___________________________________ 0\n",
      "at______________________________________ 0\n",
      "term____________________________________ 0\n",
      "remained________________________________ 0\n",
      "controlled______________________________ 0\n",
      "in______________________________________ 0\n",
      "30______________________________________ 0\n",
      "(_______________________________________ 0\n",
      "88______________________________________ 0\n",
      "2_______________________________________ 0\n",
      "%_______________________________________ 0\n",
      ")_______________________________________ 0\n",
      "patients________________________________ 0\n",
      "in______________________________________ 0\n",
      "group___________________________________ 0\n",
      "-_______________________________________ 0\n",
      "a_______________________________________ 0\n",
      "and_____________________________________ 0\n",
      "27______________________________________ 0\n",
      "(_______________________________________ 0\n",
      "79______________________________________ 0\n",
      "4_______________________________________ 0\n",
      "%_______________________________________ 0\n",
      ")_______________________________________ 0\n",
      "in______________________________________ 0\n",
      "group___________________________________ 0\n",
      "-_______________________________________ 0\n",
      "b_______________________________________ 0\n",
      "(_______________________________________ 0\n",
      "p_______________________________________ 0\n",
      "0_______________________________________ 0\n",
      "05______________________________________ 0\n",
      ")_______________________________________ 0\n",
      "comparison______________________________ 0\n",
      "of______________________________________ 0\n",
      "random__________________________________ 0\n",
      "blood___________________________________ 0\n",
      "sugar___________________________________ 0\n",
      "levels__________________________________ 0\n",
      "within__________________________________ 0\n",
      "normal__________________________________ 0\n",
      "limits__________________________________ 0\n",
      "after___________________________________ 0\n",
      "1_______________________________________ 0\n",
      "month___________________________________ 0\n",
      "in______________________________________ 0\n",
      "25______________________________________ 0\n",
      "(_______________________________________ 0\n",
      "73______________________________________ 0\n",
      "5_______________________________________ 0\n",
      "%_______________________________________ 0\n",
      ")_______________________________________ 0\n",
      "in______________________________________ 0\n",
      "group___________________________________ 0\n",
      "-_______________________________________ 0\n",
      "a_______________________________________ 0\n",
      "and_____________________________________ 0\n",
      "in______________________________________ 0\n",
      "24______________________________________ 0\n",
      "(_______________________________________ 0\n",
      "70______________________________________ 0\n",
      "6_______________________________________ 0\n",
      "%_______________________________________ 0\n",
      ")_______________________________________ 0\n",
      "in______________________________________ 0\n",
      "group___________________________________ 0\n",
      "-_______________________________________ 0\n",
      "b_______________________________________ 0\n",
      "at______________________________________ 0\n",
      "term____________________________________ 0\n",
      "random__________________________________ 0\n",
      "blood___________________________________ 0\n",
      "sugar___________________________________ 0\n",
      "level___________________________________ 0\n",
      "was_____________________________________ 0\n",
      "controlled______________________________ 0\n",
      "in______________________________________ 0\n",
      "28______________________________________ 0\n",
      "(_______________________________________ 0\n",
      "82______________________________________ 0\n",
      "4_______________________________________ 0\n",
      "%_______________________________________ 0\n",
      ")_______________________________________ 0\n",
      "and_____________________________________ 0\n",
      "27______________________________________ 0\n",
      "(_______________________________________ 0\n",
      "79______________________________________ 0\n",
      "4_______________________________________ 0\n",
      "%_______________________________________ 0\n",
      ")_______________________________________ 0\n",
      "patients________________________________ 0\n",
      "in______________________________________ 0\n",
      "group___________________________________ 0\n",
      "-_______________________________________ 0\n",
      "a_______________________________________ 0\n",
      "and_____________________________________ 0\n",
      "b_______________________________________ 0\n",
      "respectively____________________________ 0\n",
      "comparison______________________________ 0\n",
      "of______________________________________ 0\n",
      "post____________________________________ 0\n",
      "-_______________________________________ 0\n",
      "treatment_______________________________ 0\n",
      "hba1c___________________________________ 0\n",
      "level___________________________________ 0\n",
      "depic___________________________________ 0\n",
      "##ts____________________________________ 0\n",
      "that____________________________________ 0\n",
      "diabetes________________________________ 1\n",
      "controlled______________________________ 0\n",
      "in______________________________________ 0\n",
      "27______________________________________ 0\n",
      "(_______________________________________ 0\n",
      "79______________________________________ 0\n",
      "4_______________________________________ 0\n",
      "%_______________________________________ 0\n",
      ")_______________________________________ 0\n",
      "patients________________________________ 0\n",
      "in______________________________________ 0\n",
      "group___________________________________ 0\n",
      "-_______________________________________ 0\n",
      "a_______________________________________ 0\n",
      "while___________________________________ 0\n",
      "in______________________________________ 0\n",
      "28______________________________________ 0\n",
      "(_______________________________________ 0\n",
      "82______________________________________ 0\n",
      "3_______________________________________ 0\n",
      "%_______________________________________ 0\n",
      ")_______________________________________ 0\n",
      "patients________________________________ 0\n",
      "of______________________________________ 0\n",
      "group___________________________________ 0\n",
      "-_______________________________________ 0\n",
      "b_______________________________________ 0\n",
      "the_____________________________________ 0\n",
      "efficacy________________________________ 0\n",
      "of______________________________________ 0\n",
      "metformin_______________________________ 9\n",
      "and_____________________________________ 0\n",
      "insulin_________________________________ 7\n",
      "in______________________________________ 0\n",
      "controlling_____________________________ 0\n",
      "diabetes________________________________ 1\n",
      "was_____________________________________ 0\n",
      "equal___________________________________ 0\n",
      "in______________________________________ 0\n",
      "two_____________________________________ 0\n",
      "groups__________________________________ 0\n",
      "conclusion______________________________ 0\n",
      "there___________________________________ 0\n",
      "was_____________________________________ 0\n",
      "no______________________________________ 0\n",
      "marked__________________________________ 0\n",
      "difference______________________________ 0\n",
      "in______________________________________ 0\n",
      "efficacy________________________________ 0\n",
      "of______________________________________ 0\n",
      "metformin_______________________________ 9\n",
      "and_____________________________________ 0\n",
      "insulin_________________________________ 7\n",
      "in______________________________________ 0\n",
      "controlling_____________________________ 0\n",
      "diabetes________________________________ 1\n",
      "in______________________________________ 0\n",
      "pregnant________________________________ 0\n",
      "patients________________________________ 0\n",
      "in______________________________________ 0\n",
      "two_____________________________________ 0\n",
      "groups__________________________________ 0\n",
      "[SEP]___________________________________ -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(q['input_ids'][0]), q['labels'][0]):\n",
    "    print(f'{token:_<40} {label}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1394/1394 [00:05<00:00, 262.67 examples/s]\n",
      "Map: 100%|██████████| 78/78 [00:00<00:00, 243.58 examples/s]\n",
      "Map: 100%|██████████| 77/77 [00:00<00:00, 262.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = part_train_test.map(tokenize_align_labels, batched = True, remove_columns=part_train_test['train'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r'C:\\Users\\Gbadamosi\\Documents\\Nerd Corner\\Master in ds and AI\\MSC project\\workspace\\workspace\\ner\\models\\BioElectra\\thesis\\abstract'\n",
    "run_name = \"Bioelectra_Abstract\"\n",
    "from transformers import Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir= output_dir,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=1,\n",
    "    save_strategy = 'epoch',\n",
    "    learning_rate = learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=2,  \n",
    "    run_name =run_name,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    args,\n",
    "    train_dataset = tokenized_dataset['train'],\n",
    "    eval_dataset = tokenized_dataset['valid'],\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    \n",
    "\n",
    "    compute_metrics = compute_metrics_by_token_swt\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1394\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 78\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 77\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 87/435 [21:19<1:17:44, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1622, 'learning_rate': 4e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\virtual environments\\natural_language_1.0\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 20%|██        | 87/435 [21:33<1:17:44, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23381659388542175, 'eval_f1_B-cond': 0.47750865051903113, 'eval_f1_I-cond': 0.5030674846625767, 'eval_f1_B-des': 0.6125, 'eval_f1_I-des': 0.7277486910994765, 'eval_f1_B-subj': 0.5771428571428572, 'eval_f1_I-subj': 0.5140845070422535, 'eval_f1_B-group_A': 0.464746772591857, 'eval_f1_I-group_A': 0.2026431718061674, 'eval_f1_B-group_B': 0.373806275579809, 'eval_f1_I-group_B': 0.2097902097902098, 'eval_f1_B-group_C': 0.0, 'eval_f1_I-group_C': 0.0, 'eval_f1_B-group_D': 0.0, 'eval_f1_I-group_D': 0.0, 'eval_runtime': 14.6469, 'eval_samples_per_second': 5.257, 'eval_steps_per_second': 1.365, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 174/435 [42:02<1:04:34, 14.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1261, 'learning_rate': 3e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\virtual environments\\natural_language_1.0\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 40%|████      | 174/435 [42:19<1:04:34, 14.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2520667612552643, 'eval_f1_B-cond': 0.5101449275362319, 'eval_f1_I-cond': 0.5156537753222836, 'eval_f1_B-des': 0.6338797814207651, 'eval_f1_I-des': 0.7174887892376682, 'eval_f1_B-subj': 0.5867970660146699, 'eval_f1_I-subj': 0.5705128205128205, 'eval_f1_B-group_A': 0.536007292616226, 'eval_f1_I-group_A': 0.4431486880466472, 'eval_f1_B-group_B': 0.38014527845036317, 'eval_f1_I-group_B': 0.2180094786729858, 'eval_f1_B-group_C': 0.0, 'eval_f1_I-group_C': 0.0, 'eval_f1_B-group_D': 0.0, 'eval_f1_I-group_D': 0.0, 'eval_runtime': 16.4972, 'eval_samples_per_second': 4.667, 'eval_steps_per_second': 1.212, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 261/435 [1:03:09<39:23, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1034, 'learning_rate': 2e-05, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\virtual environments\\natural_language_1.0\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Gbadamosi\\AppData\\Local\\Temp\\ipykernel_17620\\1388153939.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return 2 * prec * rec / (prec + rec)\n",
      "\n",
      " 60%|██████    | 261/435 [1:03:30<39:23, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25815680623054504, 'eval_f1_B-cond': 0.43621399176954734, 'eval_f1_I-cond': 0.4694835680751174, 'eval_f1_B-des': 0.5241379310344828, 'eval_f1_I-des': 0.6542553191489362, 'eval_f1_B-subj': 0.5558912386706948, 'eval_f1_I-subj': 0.5499999999999999, 'eval_f1_B-group_A': 0.46490428441203285, 'eval_f1_I-group_A': 0.42436974789915966, 'eval_f1_B-group_B': 0.38081395348837216, 'eval_f1_I-group_B': 0.2292490118577075, 'eval_f1_B-group_C': 0.13740458015267176, 'eval_f1_I-group_C': nan, 'eval_f1_B-group_D': 0.0, 'eval_f1_I-group_D': 0.0, 'eval_runtime': 20.8217, 'eval_samples_per_second': 3.698, 'eval_steps_per_second': 0.961, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 349/435 [1:24:33<19:59, 13.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0867, 'learning_rate': 9.885057471264368e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\virtual environments\\natural_language_1.0\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Gbadamosi\\AppData\\Local\\Temp\\ipykernel_17620\\1388153939.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return 2 * prec * rec / (prec + rec)\n",
      "\n",
      " 80%|████████  | 349/435 [1:24:46<19:59, 13.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2720515727996826, 'eval_f1_B-cond': 0.51875, 'eval_f1_I-cond': 0.50561797752809, 'eval_f1_B-des': 0.5925925925925926, 'eval_f1_I-des': 0.6490765171503958, 'eval_f1_B-subj': 0.5272727272727272, 'eval_f1_I-subj': 0.5968253968253967, 'eval_f1_B-group_A': 0.4908424908424908, 'eval_f1_I-group_A': 0.4313725490196078, 'eval_f1_B-group_B': 0.4050632911392405, 'eval_f1_I-group_B': 0.22656249999999997, 'eval_f1_B-group_C': 0.16370106761565836, 'eval_f1_I-group_C': nan, 'eval_f1_B-group_D': 0.0, 'eval_f1_I-group_D': 0.0, 'eval_runtime': 12.3934, 'eval_samples_per_second': 6.213, 'eval_steps_per_second': 1.614, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 435/435 [1:45:42<00:00, 14.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0811, 'learning_rate': 0.0, 'epoch': 4.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\virtual environments\\natural_language_1.0\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Gbadamosi\\AppData\\Local\\Temp\\ipykernel_17620\\1388153939.py:32: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return 2 * prec * rec / (prec + rec)\n",
      "\n",
      "100%|██████████| 435/435 [1:45:54<00:00, 14.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2779647409915924, 'eval_f1_B-cond': 0.5177993527508091, 'eval_f1_I-cond': 0.5190839694656489, 'eval_f1_B-des': 0.6309523809523809, 'eval_f1_I-des': 0.6735751295336787, 'eval_f1_B-subj': 0.5714285714285714, 'eval_f1_I-subj': 0.6178343949044586, 'eval_f1_B-group_A': 0.49539594843462237, 'eval_f1_I-group_A': 0.4413145539906103, 'eval_f1_B-group_B': 0.40392706872370265, 'eval_f1_I-group_B': 0.2113207547169811, 'eval_f1_B-group_C': 0.23776223776223776, 'eval_f1_I-group_C': nan, 'eval_f1_B-group_D': 0.0, 'eval_f1_I-group_D': 0.0, 'eval_runtime': 12.452, 'eval_samples_per_second': 6.184, 'eval_steps_per_second': 1.606, 'epoch': 4.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 435/435 [1:46:01<00:00, 14.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6361.4068, 'train_samples_per_second': 1.096, 'train_steps_per_second': 0.068, 'train_loss': 0.11193458842135023, 'epoch': 4.99}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('SWT_bioelectra_tokenizer_15-11-2023\\\\tokenizer_config.json',\n",
       " 'SWT_bioelectra_tokenizer_15-11-2023\\\\special_tokens_map.json',\n",
       " 'SWT_bioelectra_tokenizer_15-11-2023\\\\vocab.txt',\n",
       " 'SWT_bioelectra_tokenizer_15-11-2023\\\\added_tokens.json',\n",
       " 'SWT_bioelectra_tokenizer_15-11-2023\\\\tokenizer.json')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('SWT_bioelectra_model_15-11-2023')\n",
    "tokenizer.save_pretrained('SWT_bioelectra_tokenizer_15-11-2023')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.20650240778923035,\n",
       " 'eval_f1_B-cond': 0.5723905723905723,\n",
       " 'eval_f1_I-cond': 0.6702127659574468,\n",
       " 'eval_f1_B-des': 0.5694444444444443,\n",
       " 'eval_f1_I-des': 0.7025089605734767,\n",
       " 'eval_f1_B-subj': 0.49382716049382713,\n",
       " 'eval_f1_I-subj': 0.5943396226415095,\n",
       " 'eval_f1_B-group_A': 0.5092322643343052,\n",
       " 'eval_f1_I-group_A': 0.16101694915254236,\n",
       " 'eval_f1_B-group_B': 0.5364583333333334,\n",
       " 'eval_f1_I-group_B': 0.4358974358974359,\n",
       " 'eval_f1_B-group_C': 0.0,\n",
       " 'eval_f1_I-group_C': 0.0,\n",
       " 'eval_f1_B-group_D': 0.0,\n",
       " 'eval_f1_I-group_D': 0.0,\n",
       " 'eval_runtime': 17.4262,\n",
       " 'eval_samples_per_second': 4.476,\n",
       " 'eval_steps_per_second': 1.148,\n",
       " 'epoch': 4.99}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset= tokenized_dataset['test'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Semaglutide, a glucagon-like peptide-1 receptor agonist, has been shown to reduce the risk of adverse cardiovascular events in patients with diabetes. Whether semaglutide can reduce cardiovascular risk associated with overweight and obesity in the absence of diabetes is unknown'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Semaglutide, a glucagon-like peptide-1 receptor agonist, has been shown to reduce the risk of adverse cardiovascular events in patients with diabetes. Whether semaglutide can reduce cardiovascular risk associated with overweight and obesity in the absence of diabetes is unknown\"\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using checkpoint C:\\Users\\Gbadamosi\\Documents\\Nerd Corner\\Master in ds and AI\\MSC project\\workspace\\workspace\\ner\\models\\BioElectra\\thesis\\abstract\\checkpoint-349\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = trainer.state.best_model_checkpoint # or save model in disk and load it later\n",
    "print(f\"using checkpoint {model_checkpoint}\")\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\", model=model_checkpoint, aggregation_strategy=\"first\")\n",
    "# token_classifier.tokenizer.model_max_length = model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'group_A', 'score': 0.64658564, 'word': 'semaglutide', 'start': 0, 'end': 11}, {'entity_group': 'group_A', 'score': 0.68428516, 'word': 'semaglutide', 'start': 159, 'end': 170}]\n"
     ]
    }
   ],
   "source": [
    "res = token_classifier(sentence, aggregation_strategy=\"first\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ABSTRACT AND METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Huggingface Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 1542/1542 [00:00<00:00, 24672.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "hf_ds = create_datasets(abstract_methods, tru_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pmid': Value(dtype='string', id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-cond', 'I-cond', 'B-des', 'I-des', 'B-subj', 'I-subj', 'B-group_A', 'I-group_A', 'B-group_B', 'I-group_B', 'B-group_C', 'I-group_C', 'B-group_D', 'I-group_D'], id=None), length=-1, id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pmid': '16960863',\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  11,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  9,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  11,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  9,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  9,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  0,\n",
       "  11,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'tokens': ['effects',\n",
       "  'of',\n",
       "  'rivastigmine',\n",
       "  'in',\n",
       "  'patients',\n",
       "  'with',\n",
       "  'and',\n",
       "  'without',\n",
       "  'visual',\n",
       "  'hallucinations',\n",
       "  'in',\n",
       "  'dementia',\n",
       "  'associated',\n",
       "  'with',\n",
       "  'parkinsons',\n",
       "  'disease',\n",
       "  'we',\n",
       "  'aimed',\n",
       "  'to',\n",
       "  'determine',\n",
       "  'prospectively',\n",
       "  'whether',\n",
       "  'rivastigmine',\n",
       "  'an',\n",
       "  'inhibitor',\n",
       "  'of',\n",
       "  'acetylcholinesterase',\n",
       "  'and',\n",
       "  'butyrylcholinesterase',\n",
       "  'provided',\n",
       "  'benefits',\n",
       "  'in',\n",
       "  'patients',\n",
       "  'with',\n",
       "  'and',\n",
       "  'without',\n",
       "  'visual',\n",
       "  'hallucinations',\n",
       "  'in',\n",
       "  'a',\n",
       "  'population',\n",
       "  'with',\n",
       "  'dementia',\n",
       "  'associated',\n",
       "  'with',\n",
       "  'parkinsons',\n",
       "  'disease',\n",
       "  'this',\n",
       "  'was',\n",
       "  'a',\n",
       "  '24-week',\n",
       "  'double-blind',\n",
       "  'placebo-controlled',\n",
       "  'study',\n",
       "  'primary',\n",
       "  'efficacy',\n",
       "  'measures',\n",
       "  'were',\n",
       "  'the',\n",
       "  'alzheimers',\n",
       "  'disease',\n",
       "  'assessment',\n",
       "  'scale',\n",
       "  'cognitive',\n",
       "  'subscale',\n",
       "  '(adas-cog)',\n",
       "  'and',\n",
       "  'alzheimers',\n",
       "  'disease',\n",
       "  'cooperative',\n",
       "  'study-clinicians',\n",
       "  'global',\n",
       "  'impression',\n",
       "  'of',\n",
       "  'change',\n",
       "  '(adcs-cgic)',\n",
       "  'secondary',\n",
       "  'efficacy',\n",
       "  'measures',\n",
       "  'included',\n",
       "  'activities',\n",
       "  'of',\n",
       "  'daily',\n",
       "  'living',\n",
       "  'behavioral',\n",
       "  'symptoms',\n",
       "  'and',\n",
       "  'executive',\n",
       "  'and',\n",
       "  'attentional',\n",
       "  'functions',\n",
       "  'patients',\n",
       "  'were',\n",
       "  'stratified',\n",
       "  'according',\n",
       "  'to',\n",
       "  'the',\n",
       "  'presence',\n",
       "  'of',\n",
       "  'visual',\n",
       "  'hallucinations',\n",
       "  'at',\n",
       "  'baseline',\n",
       "  'the',\n",
       "  'study',\n",
       "  'included',\n",
       "  '188',\n",
       "  'visual',\n",
       "  'hallucinators',\n",
       "  '(118',\n",
       "  'on',\n",
       "  'rivastigmine',\n",
       "  '70',\n",
       "  'on',\n",
       "  'placebo)',\n",
       "  'and',\n",
       "  '348',\n",
       "  'nonvisual',\n",
       "  'hallucinators',\n",
       "  '(239',\n",
       "  'on',\n",
       "  'rivastigmine',\n",
       "  '109',\n",
       "  'on',\n",
       "  'placebo)',\n",
       "  'rivastigmine',\n",
       "  'provided',\n",
       "  'benefits',\n",
       "  'in',\n",
       "  'both',\n",
       "  'visual',\n",
       "  'hallucinators',\n",
       "  'and',\n",
       "  'nonvisual',\n",
       "  'hallucinators',\n",
       "  'absolute',\n",
       "  'responses',\n",
       "  'to',\n",
       "  'rivastigmine',\n",
       "  'on',\n",
       "  'the',\n",
       "  'adas-cog',\n",
       "  'were',\n",
       "  'comparable',\n",
       "  'over',\n",
       "  '6',\n",
       "  'months',\n",
       "  'although',\n",
       "  'rivastigmine-placebo',\n",
       "  'differences',\n",
       "  'tended',\n",
       "  'to',\n",
       "  'be',\n",
       "  'larger',\n",
       "  'in',\n",
       "  'visual',\n",
       "  'hallucinators',\n",
       "  '(4',\n",
       "  '27',\n",
       "  'p',\n",
       "  '0',\n",
       "  '002)',\n",
       "  'than',\n",
       "  'in',\n",
       "  'nonhallucinators',\n",
       "  '(2',\n",
       "  '09',\n",
       "  'p',\n",
       "  '0',\n",
       "  '015)',\n",
       "  'on',\n",
       "  'the',\n",
       "  'adcs-cgic',\n",
       "  'differences',\n",
       "  'between',\n",
       "  'rivastigmine',\n",
       "  'and',\n",
       "  'placebo',\n",
       "  'were',\n",
       "  '0',\n",
       "  '5',\n",
       "  'in',\n",
       "  'visual',\n",
       "  'hallucinators',\n",
       "  '(p',\n",
       "  '0',\n",
       "  '030)',\n",
       "  'and',\n",
       "  '0',\n",
       "  '3',\n",
       "  'in',\n",
       "  'nonhallucinators',\n",
       "  '(p',\n",
       "  '0',\n",
       "  '111)',\n",
       "  'rivastigmine',\n",
       "  'provided',\n",
       "  'benefits',\n",
       "  'on',\n",
       "  'all',\n",
       "  'secondary',\n",
       "  'efficacy',\n",
       "  'measures',\n",
       "  'and',\n",
       "  'placebo',\n",
       "  'declines',\n",
       "  'and',\n",
       "  'treatment',\n",
       "  'differences',\n",
       "  'were',\n",
       "  'more',\n",
       "  'marked',\n",
       "  'in',\n",
       "  'visual',\n",
       "  'hallucinators',\n",
       "  'adverse',\n",
       "  'events',\n",
       "  'were',\n",
       "  'reported',\n",
       "  'more',\n",
       "  'frequently',\n",
       "  'by',\n",
       "  'rivastigmine-treated',\n",
       "  'patients',\n",
       "  'although',\n",
       "  'this',\n",
       "  'difference',\n",
       "  'was',\n",
       "  'less',\n",
       "  'marked',\n",
       "  'in',\n",
       "  'visual',\n",
       "  'hallucinators',\n",
       "  'visual',\n",
       "  'hallucinations',\n",
       "  'appear',\n",
       "  'to',\n",
       "  'predict',\n",
       "  'more',\n",
       "  'rapid',\n",
       "  'decline',\n",
       "  'and',\n",
       "  'possibly',\n",
       "  'greater',\n",
       "  'therapeutic',\n",
       "  'benefit',\n",
       "  'from',\n",
       "  'rivastigmine',\n",
       "  'treatment',\n",
       "  'in',\n",
       "  'pdd',\n",
       "  'study',\n",
       "  'design',\n",
       "  'the',\n",
       "  'methodology',\n",
       "  'of',\n",
       "  'the',\n",
       "  'double-blind',\n",
       "  'study',\n",
       "  'has',\n",
       "  'been',\n",
       "  'described',\n",
       "  'previously',\n",
       "  '11',\n",
       "  'patients',\n",
       "  'had',\n",
       "  'a',\n",
       "  'diagnosis',\n",
       "  'of',\n",
       "  'pd',\n",
       "  'according',\n",
       "  'to',\n",
       "  'the',\n",
       "  'queens',\n",
       "  'square',\n",
       "  'brain',\n",
       "  'bank',\n",
       "  'clinical',\n",
       "  'diagnostic',\n",
       "  'criteria12',\n",
       "  'and',\n",
       "  'dementia',\n",
       "  'due',\n",
       "  'to',\n",
       "  'pd',\n",
       "  'according',\n",
       "  'to',\n",
       "  'the',\n",
       "  'fourth',\n",
       "  'edition',\n",
       "  'of',\n",
       "  'the',\n",
       "  'diagnostic',\n",
       "  'and',\n",
       "  'statistical',\n",
       "  'manual',\n",
       "  'of',\n",
       "  'mental',\n",
       "  'disorders',\n",
       "  '(dsm-iv',\n",
       "  'code',\n",
       "  '294',\n",
       "  '1)',\n",
       "  '13',\n",
       "  'patients',\n",
       "  'had',\n",
       "  'mild',\n",
       "  'to',\n",
       "  'moderately',\n",
       "  'severe',\n",
       "  'dementia',\n",
       "  'asdefined',\n",
       "  'by',\n",
       "  'a',\n",
       "  'mini-mental',\n",
       "  'state',\n",
       "  'examination',\n",
       "  'score',\n",
       "  'of',\n",
       "  '10',\n",
       "  'to',\n",
       "  '24',\n",
       "  'patients',\n",
       "  'were',\n",
       "  'randomized',\n",
       "  'to',\n",
       "  'rivastigmine',\n",
       "  'or',\n",
       "  'placebo',\n",
       "  'in',\n",
       "  'a',\n",
       "  '2',\n",
       "  '1',\n",
       "  'ratio',\n",
       "  'which',\n",
       "  'permitted',\n",
       "  'the',\n",
       "  'collection',\n",
       "  'of',\n",
       "  'more',\n",
       "  'safety',\n",
       "  'data',\n",
       "  'in',\n",
       "  'the',\n",
       "  'rivastigmine',\n",
       "  'group',\n",
       "  'the',\n",
       "  'study',\n",
       "  'included',\n",
       "  'a',\n",
       "  '16-week',\n",
       "  'dose',\n",
       "  'escalation',\n",
       "  'phase',\n",
       "  'in',\n",
       "  'order',\n",
       "  'to',\n",
       "  'reach',\n",
       "  'maximum',\n",
       "  'tolerated',\n",
       "  'doses',\n",
       "  'of',\n",
       "  'rivastigmine',\n",
       "  'up',\n",
       "  'to',\n",
       "  '12',\n",
       "  'mg',\n",
       "  'day',\n",
       "  'which',\n",
       "  'was',\n",
       "  'maintained',\n",
       "  'for',\n",
       "  'another',\n",
       "  '8',\n",
       "  'weeks',\n",
       "  'at',\n",
       "  'baseline',\n",
       "  'and',\n",
       "  'at',\n",
       "  'weeks',\n",
       "  '16',\n",
       "  'and',\n",
       "  '24',\n",
       "  'efficacymeasures',\n",
       "  'were',\n",
       "  'performed',\n",
       "  'primary',\n",
       "  'measures',\n",
       "  'were',\n",
       "  'the',\n",
       "  'cognitive',\n",
       "  'subscale',\n",
       "  'of',\n",
       "  'the',\n",
       "  'alzheimers',\n",
       "  'disease',\n",
       "  'assessment',\n",
       "  'scale',\n",
       "  '(adas-cog)14',\n",
       "  'and',\n",
       "  'the',\n",
       "  'alzheimers',\n",
       "  'disease',\n",
       "  'cooperative',\n",
       "  'study-clinicians',\n",
       "  'global',\n",
       "  'impression',\n",
       "  'of',\n",
       "  'change',\n",
       "  '(adcscgic)',\n",
       "  '15',\n",
       "  'secondary',\n",
       "  'efficacy',\n",
       "  'parameterswere',\n",
       "  'thealzheimers',\n",
       "  'disease',\n",
       "  'cooperative',\n",
       "  'study-activities',\n",
       "  'of',\n",
       "  'daily',\n",
       "  'living',\n",
       "  '(adcs-adl)',\n",
       "  '16',\n",
       "  'verbal',\n",
       "  'fluency',\n",
       "  'tests',\n",
       "  'from',\n",
       "  'the',\n",
       "  'delis-kaplan',\n",
       "  'executive',\n",
       "  'function',\n",
       "  'system',\n",
       "  '(d-kefs)',\n",
       "  '17',\n",
       "  'power',\n",
       "  'of',\n",
       "  'attention',\n",
       "  'including',\n",
       "  'choice',\n",
       "  'reaction',\n",
       "  'time',\n",
       "  'from',\n",
       "  'the',\n",
       "  'cognitive',\n",
       "  'drug',\n",
       "  'research',\n",
       "  'attention',\n",
       "  'battery',\n",
       "  '18',\n",
       "  'the',\n",
       "  'mmse',\n",
       "  '19',\n",
       "  'and',\n",
       "  'the',\n",
       "  '10-item',\n",
       "  'neuropsychiatric',\n",
       "  'inventory',\n",
       "  '(npi-10)',\n",
       "  '20',\n",
       "  'safety',\n",
       "  'evaluations',\n",
       "  'included',\n",
       "  'recording',\n",
       "  'adverse',\n",
       "  'events',\n",
       "  'laboratory',\n",
       "  'parameters',\n",
       "  'and',\n",
       "  'ecg',\n",
       "  'extrapyramidal',\n",
       "  'signs',\n",
       "  'were',\n",
       "  'assessed',\n",
       "  'with',\n",
       "  'the',\n",
       "  'motor',\n",
       "  'subsection',\n",
       "  'of',\n",
       "  'the',\n",
       "  'unified',\n",
       "  'parkinsonsdisease',\n",
       "  'rating',\n",
       "  'scale',\n",
       "  '(updrs',\n",
       "  'part',\n",
       "  'iii)',\n",
       "  '21',\n",
       "  'the',\n",
       "  'protocol',\n",
       "  'informed',\n",
       "  'consent',\n",
       "  'form',\n",
       "  'and',\n",
       "  'to',\n",
       "  'patients',\n",
       "  'and',\n",
       "  'caregivers',\n",
       "  'were',\n",
       "  'reviewed',\n",
       "  'by',\n",
       "  'tutional',\n",
       "  'review',\n",
       "  'boards',\n",
       "  'all',\n",
       "  'procedures',\n",
       "  'w',\n",
       "  'dance',\n",
       "  'with',\n",
       "  'the',\n",
       "  'helsinki',\n",
       "  'declaration',\n",
       "  'as',\n",
       "  'rev',\n",
       "  'information',\n",
       "  'local',\n",
       "  'instiere',\n",
       "  'in',\n",
       "  'accorised',\n",
       "  'in',\n",
       "  '1983',\n",
       "  'analyses',\n",
       "  'of',\n",
       "  'response',\n",
       "  'in',\n",
       "  'visual',\n",
       "  'hallucinators',\n",
       "  'vs',\n",
       "  'nonhallucinators',\n",
       "  'this',\n",
       "  'was',\n",
       "  'a',\n",
       "  'prospective',\n",
       "  'analysis',\n",
       "  'that',\n",
       "  'was',\n",
       "  'described',\n",
       "  'in',\n",
       "  'the',\n",
       "  'protocol',\n",
       "  'the',\n",
       "  'presence',\n",
       "  'or',\n",
       "  'absence',\n",
       "  'of',\n",
       "  'any',\n",
       "  'hallucinaariables',\n",
       "  'nd',\n",
       "  'conefficacy',\n",
       "  'ded',\n",
       "  'last',\n",
       "  'for',\n",
       "  'the',\n",
       "  'itt',\n",
       "  'observed',\n",
       "  'case',\n",
       "  'tions',\n",
       "  'at',\n",
       "  'baseline',\n",
       "  'was',\n",
       "  'recorded',\n",
       "  'using',\n",
       "  'the',\n",
       "  'npi-10',\n",
       "  'caregivers',\n",
       "  'were',\n",
       "  'asked',\n",
       "  'to',\n",
       "  'tick',\n",
       "  'an',\n",
       "  'extra',\n",
       "  'box',\n",
       "  'marking',\n",
       "  'the',\n",
       "  'presence',\n",
       "  'of',\n",
       "  'visual',\n",
       "  'hallucinations',\n",
       "  'two',\n",
       "  'subgroups',\n",
       "  'were',\n",
       "  'defined',\n",
       "  'those',\n",
       "  'with',\n",
       "  'visual',\n",
       "  'hallucinations',\n",
       "  'and',\n",
       "  'those',\n",
       "  'without',\n",
       "  'visual',\n",
       "  'hallucinations',\n",
       "  '(but',\n",
       "  'who',\n",
       "  'might',\n",
       "  'have',\n",
       "  'other',\n",
       "  'kinds',\n",
       "  'of',\n",
       "  'hallucination)',\n",
       "  'at',\n",
       "  'baseline',\n",
       "  'all',\n",
       "  'patients',\n",
       "  'who',\n",
       "  'received',\n",
       "  'at',\n",
       "  'least',\n",
       "  'one',\n",
       "  'dose',\n",
       "  'of',\n",
       "  'study',\n",
       "  'medication',\n",
       "  'and',\n",
       "  'had',\n",
       "  'at',\n",
       "  'least',\n",
       "  'one',\n",
       "  'safety',\n",
       "  'measurement',\n",
       "  'after',\n",
       "  'baseline',\n",
       "  'were',\n",
       "  'included',\n",
       "  'in',\n",
       "  'safety',\n",
       "  'analyses',\n",
       "  'mainefficacy',\n",
       "  'analyses',\n",
       "  'were',\n",
       "  'based',\n",
       "  'on',\n",
       "  'an',\n",
       "  'intention-to-treat',\n",
       "  'and',\n",
       "  'retrieved',\n",
       "  'dropout',\n",
       "  '(itt',\n",
       "  'rdo)',\n",
       "  'population',\n",
       "  'defined',\n",
       "  'as',\n",
       "  'all',\n",
       "  'randomized',\n",
       "  'patients',\n",
       "  'who',\n",
       "  'received',\n",
       "  'at',\n",
       "  'least',\n",
       "  'one',\n",
       "  'dose',\n",
       "  'of',\n",
       "  'study',\n",
       "  'medication',\n",
       "  'and',\n",
       "  'had',\n",
       "  'a',\n",
       "  'least',\n",
       "  'one',\n",
       "  'assessment',\n",
       "  'on',\n",
       "  'study',\n",
       "  'drug',\n",
       "  'for',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'primary',\n",
       "  'efficacyv',\n",
       "  'and',\n",
       "  'the',\n",
       "  'patients',\n",
       "  'who',\n",
       "  'discontinued',\n",
       "  'early',\n",
       "  'a',\n",
       "  'tinued',\n",
       "  'to',\n",
       "  'attend',\n",
       "  'original',\n",
       "  'scheduled',\n",
       "  'visits',\n",
       "  'for',\n",
       "  'evaluations',\n",
       "  'supportive',\n",
       "  'analyses',\n",
       "  'inclu',\n",
       "  'observation',\n",
       "  'carried',\n",
       "  'forward',\n",
       "  '(locf',\n",
       "  'as',\n",
       "  'rdo',\n",
       "  'but',\n",
       "  'with',\n",
       "  'an',\n",
       "  'locf',\n",
       "  'imputation)',\n",
       "  'and',\n",
       "  '(oc',\n",
       "  'no',\n",
       "  'imputation)',\n",
       "  'populations',\n",
       "  'changes',\n",
       "  'from',\n",
       "  'baseline',\n",
       "  'on',\n",
       "  'the',\n",
       "  'adas-cog',\n",
       "  'adl',\n",
       "  'cdr',\n",
       "  'and',\n",
       "  'npi-10',\n",
       "  'were',\n",
       "  'analyzed',\n",
       "  'using',\n",
       "  'an',\n",
       "  'a',\n",
       "  'of',\n",
       "  'covariance',\n",
       "  'model',\n",
       "  'using',\n",
       "  'treatme',\n",
       "  'country',\n",
       "  'as',\n",
       "  'factors',\n",
       "  'and',\n",
       "  'baseline',\n",
       "  'scores',\n",
       "  'as',\n",
       "  'cov',\n",
       "  'adcs-cgic',\n",
       "  'mmse',\n",
       "  'and',\n",
       "  'd-kefs',\n",
       "  'scores',\n",
       "  'as',\n",
       "  'changes',\n",
       "  'on',\n",
       "  'individual',\n",
       "  'items',\n",
       "  'on',\n",
       "  'the',\n",
       "  'npi',\n",
       "  'were',\n",
       "  'a',\n",
       "  'using',\n",
       "  'the',\n",
       "  'van',\n",
       "  'elteren',\n",
       "  'test',\n",
       "  'controlling',\n",
       "  'for',\n",
       "  'country',\n",
       "  'the',\n",
       "  'cdr',\n",
       "  'a',\n",
       "  'composite',\n",
       "  'power',\n",
       "  'of',\n",
       "  'attention',\n",
       "  'sco',\n",
       "  'defined',\n",
       "  'that',\n",
       "  'reflected',\n",
       "  'in',\n",
       "  'everydayterms',\n",
       "  'concentration',\n",
       "  '”',\n",
       "  'percentages',\n",
       "  'of',\n",
       "  'patients',\n",
       "  'changing',\n",
       "  '(increasing',\n",
       "  'or',\n",
       "  'decreasing',\n",
       "  'dosage',\n",
       "  'or',\n",
       "  'initiating',\n",
       "  'use)',\n",
       "  'dopaminergic',\n",
       "  'medication',\n",
       "  'use',\n",
       "  'over',\n",
       "  'the',\n",
       "  'course',\n",
       "  'of',\n",
       "  'the',\n",
       "  'study',\n",
       "  'were',\n",
       "  'recorded',\n",
       "  'to',\n",
       "  'compare',\n",
       "  'medications',\n",
       "  'with',\n",
       "  'different',\n",
       "  'potencies',\n",
       "  'in',\n",
       "  'the',\n",
       "  'dopamine',\n",
       "  'agonist',\n",
       "  'category',\n",
       "  'the',\n",
       "  'daily',\n",
       "  'dose',\n",
       "  'of',\n",
       "  'each',\n",
       "  'one',\n",
       "  'was',\n",
       "  'converted',\n",
       "  'into',\n",
       "  'the',\n",
       "  'equivalent',\n",
       "  'pramipexole',\n",
       "  'dosage',\n",
       "  'based',\n",
       "  'on',\n",
       "  'midpoints',\n",
       "  'of',\n",
       "  'recommended',\n",
       "  'average',\n",
       "  'dosages',\n",
       "  'reported',\n",
       "  'in',\n",
       "  'the',\n",
       "  'literature',\n",
       "  'for',\n",
       "  'example',\n",
       "  '2',\n",
       "  '75',\n",
       "  'mg',\n",
       "  'day',\n",
       "  'of',\n",
       "  'pergolide',\n",
       "  '(range',\n",
       "  '2-3',\n",
       "  '5',\n",
       "  'mg',\n",
       "  'day)',\n",
       "  'was',\n",
       "  'considered',\n",
       "  'equivalent',\n",
       "  'to',\n",
       "  '3',\n",
       "  'mg',\n",
       "  'day',\n",
       "  'of',\n",
       "  'pramipexole',\n",
       "  '(range',\n",
       "  '1',\n",
       "  '5-',\n",
       "  '4',\n",
       "  '5',\n",
       "  'mg',\n",
       "  'day)',\n",
       "  'which',\n",
       "  'resulted',\n",
       "  'in',\n",
       "  'a',\n",
       "  'conversion',\n",
       "  'factor',\n",
       "  'of',\n",
       "  '3',\n",
       "  '2',\n",
       "  '75',\n",
       "  'thus',\n",
       "  'if',\n",
       "  'a',\n",
       "  'patient',\n",
       "  'received',\n",
       "  '5',\n",
       "  '5',\n",
       "  'mg',\n",
       "  'day',\n",
       "  'of',\n",
       "  'pergolide',\n",
       "  'it',\n",
       "  'was',\n",
       "  'considered',\n",
       "  'to',\n",
       "  'be',\n",
       "  'the',\n",
       "  'equivalent',\n",
       "  'of',\n",
       "  'taking',\n",
       "  '6',\n",
       "  'mg',\n",
       "  'day',\n",
       "  'of',\n",
       "  'pramipexole',\n",
       "  'this',\n",
       "  'is',\n",
       "  'considered',\n",
       "  'a',\n",
       "  'valid',\n",
       "  'approach',\n",
       "  'for',\n",
       "  'comparing',\n",
       "  'dosages',\n",
       "  'of',\n",
       "  'medications',\n",
       "  'with',\n",
       "  'different',\n",
       "  'potencies',\n",
       "  'adcsnalysis',\n",
       "  'nt',\n",
       "  'and',\n",
       "  'ariates',\n",
       "  'well',\n",
       "  'as',\n",
       "  'nalyzed',\n",
       "  'using',\n",
       "  're',\n",
       "  'was',\n",
       "  '“effortful']}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_ds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_train_test = train_test_split(hf_ds, train_test_size=0.1, validation_size=0.5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pmid', 'ner_tags', 'tokens'],\n",
       "        num_rows: 1387\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pmid', 'ner_tags', 'tokens'],\n",
       "        num_rows: 78\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['pmid', 'ner_tags', 'tokens'],\n",
       "        num_rows: 77\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-cond',\n",
       " 'I-cond',\n",
       " 'B-des',\n",
       " 'I-des',\n",
       " 'B-subj',\n",
       " 'I-subj',\n",
       " 'B-group_A',\n",
       " 'I-group_A',\n",
       " 'B-group_B',\n",
       " 'I-group_B',\n",
       " 'B-group_C',\n",
       " 'I-group_C',\n",
       " 'B-group_D',\n",
       " 'I-group_D']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = part_train_test['train'].features['ner_tags'].feature.names\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1387/1387 [00:18<00:00, 76.40 examples/s]\n",
      "Map: 100%|██████████| 78/78 [00:01<00:00, 75.81 examples/s]\n",
      "Map: 100%|██████████| 77/77 [00:00<00:00, 78.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = part_train_test.map(tokenize_align_labels, batched = True, remove_columns=part_train_test['train'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r'C:\\Users\\Gbadamosi\\Documents\\Nerd Corner\\Master in ds and AI\\MSC project\\workspace\\workspace\\ner\\models\\BioElectra\\thesis\\abstract_methods'\n",
    "run_name = \"Bioelectra_Abstract_Method\"\n",
    "from transformers import Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir= output_dir,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=1,\n",
    "    save_strategy = 'epoch',\n",
    "    learning_rate = learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=2,  \n",
    "    run_name =run_name,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    args,\n",
    "    train_dataset = tokenized_dataset['train'],\n",
    "    eval_dataset = tokenized_dataset['valid'],\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    \n",
    "\n",
    "    compute_metrics = compute_metrics_by_token_swt\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1387\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 78\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 77\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 66/430 [18:01<1:43:16, 17.02s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gbadamosi\\Documents\\Nerd Corner\\Master in ds and AI\\MSC project\\workspace\\workspace\\ner\\notebooks\\tests codes\\Bioelectra_SWT_NER_THESIS.ipynb Cell 85\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Gbadamosi/Documents/Nerd%20Corner/Master%20in%20ds%20and%20AI/MSC%20project/workspace/workspace/ner/notebooks/tests%20codes/Bioelectra_SWT_NER_THESIS.ipynb#Y200sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gbadamosi/Documents/Nerd%20Corner/Master%20in%20ds%20and%20AI/MSC%20project/workspace/workspace/ner/notebooks/tests%20codes/Bioelectra_SWT_NER_THESIS.ipynb#Y200sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(\u001b[39m'\u001b[39m\u001b[39mAbstract_method_SWT_bioelectra_model_18-11-2023\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gbadamosi/Documents/Nerd%20Corner/Master%20in%20ds%20and%20AI/MSC%20project/workspace/workspace/ner/notebooks/tests%20codes/Bioelectra_SWT_NER_THESIS.ipynb#Y200sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tokenizer\u001b[39m.\u001b[39msave_pretrained(\u001b[39m'\u001b[39m\u001b[39mAbstract_method_SWT_bioelectra_tokenizer_18-11-2023\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\virtual environments\\natural_language_1.0\\Lib\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1560\u001b[0m     )\n",
      "File \u001b[1;32mc:\\virtual environments\\natural_language_1.0\\Lib\\site-packages\\transformers\\trainer.py:1865\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1859\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m   1860\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1862\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1863\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1864\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m-> 1865\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39;49misinf(tr_loss_step))\n\u001b[0;32m   1866\u001b[0m ):\n\u001b[0;32m   1867\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   1869\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('Abstract_method_SWT_bioelectra_model_18-11-2023')\n",
    "tokenizer.save_pretrained('Abstract_method_SWT_bioelectra_tokenizer_18-11-2023')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:39<00:00,  1.84s/it]c:\\virtual environments\\natural_language_1.0\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 20/20 [00:40<00:00,  2.01s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2409309297800064,\n",
       " 'eval_precision': 0.436607892527288,\n",
       " 'eval_recall': 0.3123123123123123,\n",
       " 'eval_f1': 0.36414565826330536,\n",
       " 'eval_accuracy': 0.9334202591475838,\n",
       " 'eval_runtime': 42.275,\n",
       " 'eval_samples_per_second': 1.845,\n",
       " 'eval_steps_per_second': 0.473,\n",
       " 'epoch': 2.97}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset= tokenized_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We aimed to determine prospectively whether rivastigmine, an inhibitor of acetylcholinesterase and butyrylcholinesterase, provided benefits in patients with and without visual hallucinations in a population with dementia associated with Parkinson's disease (PDD)\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Semaglutide, a glucagon-like peptide-1 receptor agonist, has been shown to reduce the risk of adverse cardiovascular events in patients with diabetes. Whether semaglutide can reduce cardiovascular risk associated with overweight and obesity in the absence of diabetes is unknown\"\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using checkpoint C:\\Users\\Gbadamosi\\Documents\\Nerd Corner\\Master in ds and AI\\MSC project\\workspace\\workspace\\ner\\models\\BioElectra\\thesis\\checkpoint-258\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = trainer.state.best_model_checkpoint # or save model in disk and load it later\n",
    "print(f\"using checkpoint {model_checkpoint}\")\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\", model=model_checkpoint, aggregation_strategy=\"first\")\n",
    "# token_classifier.tokenizer.model_max_length = model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "res = token_classifier(sentence, aggregation_strategy=\"first\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ABSTRACT AND METHODS SPLITTED INTO CHUNKS OF 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Huggingface Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 13880/13880 [00:00<00:00, 91142.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "hf_ds = create_datasets(splitted_50_abstract_methods_context, tru_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pmid': Value(dtype='string', id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-cond', 'I-cond', 'B-des', 'I-des', 'B-subj', 'I-subj', 'B-group_A', 'I-group_A', 'B-group_B', 'I-group_B', 'B-group_C', 'I-group_C', 'B-group_D', 'I-group_D'], id=None), length=-1, id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pmid': '16960863',\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0],\n",
       " 'tokens': ['effects',\n",
       "  'of',\n",
       "  'rivastigmine',\n",
       "  'in',\n",
       "  'patients',\n",
       "  'with',\n",
       "  'and',\n",
       "  'without',\n",
       "  'visual',\n",
       "  'hallucinations',\n",
       "  'in',\n",
       "  'dementia',\n",
       "  'associated',\n",
       "  'with',\n",
       "  'parkinsons',\n",
       "  'disease',\n",
       "  'we',\n",
       "  'aimed',\n",
       "  'to',\n",
       "  'determine',\n",
       "  'prospectively',\n",
       "  'whether',\n",
       "  'rivastigmine',\n",
       "  'an',\n",
       "  'inhibitor',\n",
       "  'of',\n",
       "  'acetylcholinesterase',\n",
       "  'and',\n",
       "  'butyrylcholinesterase',\n",
       "  'provided',\n",
       "  'benefits',\n",
       "  'in',\n",
       "  'patients',\n",
       "  'with',\n",
       "  'and',\n",
       "  'without',\n",
       "  'visual',\n",
       "  'hallucinations',\n",
       "  'in',\n",
       "  'a',\n",
       "  'population',\n",
       "  'with',\n",
       "  'dementia',\n",
       "  'associated',\n",
       "  'with',\n",
       "  'parkinsons',\n",
       "  'disease',\n",
       "  'this',\n",
       "  'was']}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_ds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_train_test = train_test_split(hf_ds, train_test_size=0.1, validation_size=0.5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pmid', 'ner_tags', 'tokens'],\n",
       "        num_rows: 12492\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pmid', 'ner_tags', 'tokens'],\n",
       "        num_rows: 694\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['pmid', 'ner_tags', 'tokens'],\n",
       "        num_rows: 694\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-cond',\n",
       " 'I-cond',\n",
       " 'B-des',\n",
       " 'I-des',\n",
       " 'B-subj',\n",
       " 'I-subj',\n",
       " 'B-group_A',\n",
       " 'I-group_A',\n",
       " 'B-group_B',\n",
       " 'I-group_B',\n",
       " 'B-group_C',\n",
       " 'I-group_C',\n",
       " 'B-group_D',\n",
       " 'I-group_D']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = part_train_test['train'].features['ner_tags'].feature.names\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12492/12492 [00:09<00:00, 1333.02 examples/s]\n",
      "Map: 100%|██████████| 694/694 [00:04<00:00, 158.05 examples/s]\n",
      "Map: 100%|██████████| 694/694 [00:00<00:00, 1444.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = part_train_test.map(tokenize_align_labels, batched = True, remove_columns=part_train_test['train'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r'C:\\Users\\Gbadamosi\\Documents\\Nerd Corner\\Master in ds and AI\\MSC project\\workspace\\workspace\\ner\\models\\BioElectra\\thesis\\fifty_abstract_methods'\n",
    "run_name = 'bio_electra_50_method_abstract'\n",
    "from transformers import Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir= output_dir,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=1,\n",
    "    save_strategy = 'epoch',\n",
    "    learning_rate = learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=2,  \n",
    "    run_name =run_name,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    args,\n",
    "    train_dataset = tokenized_dataset['train'],\n",
    "    eval_dataset = tokenized_dataset['valid'],\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    \n",
    "\n",
    "    compute_metrics = compute_metrics_by_token_swt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 12492\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 694\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 694\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2340 [00:23<7:30:32, 11.56s/it]\n",
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1714, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [00:31<00:00,  5.72it/s]c:\\virtual environments\\natural_language_1.0\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                 \n",
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19292329251766205, 'eval_precision': 0.4903912148249828, 'eval_recall': 0.6125160737248179, 'eval_f1': 0.5446922050695636, 'eval_accuracy': 0.9295361966697635, 'eval_runtime': 32.4974, 'eval_samples_per_second': 21.356, 'eval_steps_per_second': 5.354, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1422, 'learning_rate': 6.6581196581196584e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [00:32<00:00,  5.09it/s]c:\\virtual environments\\natural_language_1.0\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                 \n",
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18915516138076782, 'eval_precision': 0.5213554535827745, 'eval_recall': 0.6330904414916416, 'eval_f1': 0.5718157181571816, 'eval_accuracy': 0.9328235546344601, 'eval_runtime': 33.1299, 'eval_samples_per_second': 20.948, 'eval_steps_per_second': 5.252, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1225, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [00:26<00:00,  5.13it/s]c:\\virtual environments\\natural_language_1.0\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                 \n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19120648503303528, 'eval_precision': 0.5202821869488536, 'eval_recall': 0.632233176168024, 'eval_f1': 0.5708204334365325, 'eval_accuracy': 0.933395269063103, 'eval_runtime': 28.4008, 'eval_samples_per_second': 24.436, 'eval_steps_per_second': 6.127, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2340/2340 [1:24:20<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 5060.7878, 'train_samples_per_second': 7.405, 'train_steps_per_second': 0.462, 'train_loss': 0.14538126562395667, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Abstract_method_50_SWT_bioelectra_tokenizer_15-11-2023\\\\tokenizer_config.json',\n",
       " 'Abstract_method_50_SWT_bioelectra_tokenizer_15-11-2023\\\\special_tokens_map.json',\n",
       " 'Abstract_method_50_SWT_bioelectra_tokenizer_15-11-2023\\\\vocab.txt',\n",
       " 'Abstract_method_50_SWT_bioelectra_tokenizer_15-11-2023\\\\added_tokens.json',\n",
       " 'Abstract_method_50_SWT_bioelectra_tokenizer_15-11-2023\\\\tokenizer.json')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('Abstract_method_50_SWT_bioelectra_model_15-11-2023')\n",
    "tokenizer.save_pretrained('Abstract_method_50_SWT_bioelectra_tokenizer_15-11-2023')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [00:27<00:00,  6.45it/s]c:\\virtual environments\\natural_language_1.0\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 174/174 [00:28<00:00,  6.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.17110401391983032,\n",
       " 'eval_precision': 0.5381932333449599,\n",
       " 'eval_recall': 0.6560374149659864,\n",
       " 'eval_f1': 0.5913010155202146,\n",
       " 'eval_accuracy': 0.937537081424876,\n",
       " 'eval_runtime': 28.6072,\n",
       " 'eval_samples_per_second': 24.26,\n",
       " 'eval_steps_per_second': 6.082,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset= tokenized_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We aimed to determine prospectively whether rivastigmine, an inhibitor of acetylcholinesterase and butyrylcholinesterase, provided benefits in patients with and without visual hallucinations in a population with dementia associated with Parkinson's disease (PDD)\""
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Semaglutide, a glucagon-like peptide-1 receptor agonist, has been shown to reduce the risk of adverse cardiovascular events in patients with diabetes. Whether semaglutide can reduce cardiovascular risk associated with overweight and obesity in the absence of diabetes is unknown\"\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using checkpoint C:\\Users\\Gbadamosi\\Documents\\Nerd Corner\\Master in ds and AI\\MSC project\\workspace\\workspace\\ner\\models\\BioElectra\\thesis\\fifty_abstract_methods\\checkpoint-1561\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = trainer.state.best_model_checkpoint # or save model in disk and load it later\n",
    "print(f\"using checkpoint {model_checkpoint}\")\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\", model=model_checkpoint, aggregation_strategy=\"first\")\n",
    "# token_classifier.tokenizer.model_max_length = model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'group_A', 'score': 0.8458064, 'word': 'rivastigmine', 'start': 44, 'end': 56}, {'entity_group': 'subj', 'score': 0.8219936, 'word': 'patients', 'start': 143, 'end': 151}, {'entity_group': 'cond', 'score': 0.524707, 'word': 'visual hallucinations', 'start': 169, 'end': 190}, {'entity_group': 'cond', 'score': 0.70852387, 'word': \"dementia associated with parkinson ' s disease ( pdd )\", 'start': 212, 'end': 262}]\n"
     ]
    }
   ],
   "source": [
    "res = token_classifier(sentence, aggregation_strategy=\"first\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ABSTRACT AND METHODS SPLITTED INTO CHUNKS OF 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Huggingface Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 13880/13880 [00:00<00:00, 91142.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "hf_ds = create_datasets(splitted_512_records_method_context, tru_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pmid': Value(dtype='string', id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-cond', 'I-cond', 'B-des', 'I-des', 'B-subj', 'I-subj', 'B-group_A', 'I-group_A', 'B-group_B', 'I-group_B', 'B-group_C', 'I-group_C', 'B-group_D', 'I-group_D'], id=None), length=-1, id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pmid': '16960863',\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0],\n",
       " 'tokens': ['effects',\n",
       "  'of',\n",
       "  'rivastigmine',\n",
       "  'in',\n",
       "  'patients',\n",
       "  'with',\n",
       "  'and',\n",
       "  'without',\n",
       "  'visual',\n",
       "  'hallucinations',\n",
       "  'in',\n",
       "  'dementia',\n",
       "  'associated',\n",
       "  'with',\n",
       "  'parkinsons',\n",
       "  'disease',\n",
       "  'we',\n",
       "  'aimed',\n",
       "  'to',\n",
       "  'determine',\n",
       "  'prospectively',\n",
       "  'whether',\n",
       "  'rivastigmine',\n",
       "  'an',\n",
       "  'inhibitor',\n",
       "  'of',\n",
       "  'acetylcholinesterase',\n",
       "  'and',\n",
       "  'butyrylcholinesterase',\n",
       "  'provided',\n",
       "  'benefits',\n",
       "  'in',\n",
       "  'patients',\n",
       "  'with',\n",
       "  'and',\n",
       "  'without',\n",
       "  'visual',\n",
       "  'hallucinations',\n",
       "  'in',\n",
       "  'a',\n",
       "  'population',\n",
       "  'with',\n",
       "  'dementia',\n",
       "  'associated',\n",
       "  'with',\n",
       "  'parkinsons',\n",
       "  'disease',\n",
       "  'this',\n",
       "  'was']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_ds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_train_test = train_test_split(hf_ds, train_test_size=0.1, validation_size=0.5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pmid', 'ner_tags', 'tokens'],\n",
       "        num_rows: 12492\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pmid', 'ner_tags', 'tokens'],\n",
       "        num_rows: 694\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['pmid', 'ner_tags', 'tokens'],\n",
       "        num_rows: 694\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "part_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-cond',\n",
       " 'I-cond',\n",
       " 'B-des',\n",
       " 'I-des',\n",
       " 'B-subj',\n",
       " 'I-subj',\n",
       " 'B-group_A',\n",
       " 'I-group_A',\n",
       " 'B-group_B',\n",
       " 'I-group_B',\n",
       " 'B-group_C',\n",
       " 'I-group_C',\n",
       " 'B-group_D',\n",
       " 'I-group_D']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tags = part_train_test['train'].features['ner_tags'].feature.names\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12492/12492 [00:09<00:00, 1333.02 examples/s]\n",
      "Map: 100%|██████████| 694/694 [00:04<00:00, 158.05 examples/s]\n",
      "Map: 100%|██████████| 694/694 [00:00<00:00, 1444.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = part_train_test.map(tokenize_align_labels, batched = True, remove_columns=part_train_test['train'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r'C:\\Users\\Gbadamosi\\Documents\\Nerd Corner\\Master in ds and AI\\MSC project\\workspace\\workspace\\ner\\models\\BioElectra\\thesis\\five_hundred_abstract_methods'\n",
    "run_name = 'bio_electra_500_method_abstract'\n",
    "from transformers import Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir= output_dir,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=1,\n",
    "    save_strategy = 'epoch',\n",
    "    learning_rate = learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=2,  \n",
    "    run_name =run_name,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    args,\n",
    "    train_dataset = tokenized_dataset['train'],\n",
    "    eval_dataset = tokenized_dataset['valid'],\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    \n",
    "\n",
    "    compute_metrics = compute_metrics_by_token_swt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 12492\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 694\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 694\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2340 [00:23<7:30:32, 11.56s/it]\n",
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1714, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [00:31<00:00,  5.72it/s]c:\\virtual environments\\natural_language_1.0\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                 \n",
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19292329251766205, 'eval_precision': 0.4903912148249828, 'eval_recall': 0.6125160737248179, 'eval_f1': 0.5446922050695636, 'eval_accuracy': 0.9295361966697635, 'eval_runtime': 32.4974, 'eval_samples_per_second': 21.356, 'eval_steps_per_second': 5.354, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1422, 'learning_rate': 6.6581196581196584e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [00:32<00:00,  5.09it/s]c:\\virtual environments\\natural_language_1.0\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                 \n",
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18915516138076782, 'eval_precision': 0.5213554535827745, 'eval_recall': 0.6330904414916416, 'eval_f1': 0.5718157181571816, 'eval_accuracy': 0.9328235546344601, 'eval_runtime': 33.1299, 'eval_samples_per_second': 20.948, 'eval_steps_per_second': 5.252, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1225, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [00:26<00:00,  5.13it/s]c:\\virtual environments\\natural_language_1.0\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                 \n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19120648503303528, 'eval_precision': 0.5202821869488536, 'eval_recall': 0.632233176168024, 'eval_f1': 0.5708204334365325, 'eval_accuracy': 0.933395269063103, 'eval_runtime': 28.4008, 'eval_samples_per_second': 24.436, 'eval_steps_per_second': 6.127, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2340/2340 [1:24:20<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 5060.7878, 'train_samples_per_second': 7.405, 'train_steps_per_second': 0.462, 'train_loss': 0.14538126562395667, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Abstract_method_50_SWT_bioelectra_tokenizer_15-11-2023\\\\tokenizer_config.json',\n",
       " 'Abstract_method_50_SWT_bioelectra_tokenizer_15-11-2023\\\\special_tokens_map.json',\n",
       " 'Abstract_method_50_SWT_bioelectra_tokenizer_15-11-2023\\\\vocab.txt',\n",
       " 'Abstract_method_50_SWT_bioelectra_tokenizer_15-11-2023\\\\added_tokens.json',\n",
       " 'Abstract_method_50_SWT_bioelectra_tokenizer_15-11-2023\\\\tokenizer.json')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('Abstract_method_500_SWT_bioelectra_model_18-11-2023')\n",
    "tokenizer.save_pretrained('Abstract_method_500_SWT_bioelectra_tokenizer_18-11-2023')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [00:27<00:00,  6.45it/s]c:\\virtual environments\\natural_language_1.0\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 174/174 [00:28<00:00,  6.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.17110401391983032,\n",
       " 'eval_precision': 0.5381932333449599,\n",
       " 'eval_recall': 0.6560374149659864,\n",
       " 'eval_f1': 0.5913010155202146,\n",
       " 'eval_accuracy': 0.937537081424876,\n",
       " 'eval_runtime': 28.6072,\n",
       " 'eval_samples_per_second': 24.26,\n",
       " 'eval_steps_per_second': 6.082,\n",
       " 'epoch': 3.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset= tokenized_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We aimed to determine prospectively whether rivastigmine, an inhibitor of acetylcholinesterase and butyrylcholinesterase, provided benefits in patients with and without visual hallucinations in a population with dementia associated with Parkinson's disease (PDD)\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = \"Semaglutide, a glucagon-like peptide-1 receptor agonist, has been shown to reduce the risk of adverse cardiovascular events in patients with diabetes. Whether semaglutide can reduce cardiovascular risk associated with overweight and obesity in the absence of diabetes is unknown\"\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using checkpoint C:\\Users\\Gbadamosi\\Documents\\Nerd Corner\\Master in ds and AI\\MSC project\\workspace\\workspace\\ner\\models\\BioElectra\\thesis\\fifty_abstract_methods\\checkpoint-1561\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = trainer.state.best_model_checkpoint # or save model in disk and load it later\n",
    "print(f\"using checkpoint {model_checkpoint}\")\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\", model=model_checkpoint, aggregation_strategy=\"first\")\n",
    "# token_classifier.tokenizer.model_max_length = model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'group_A', 'score': 0.8458064, 'word': 'rivastigmine', 'start': 44, 'end': 56}, {'entity_group': 'subj', 'score': 0.8219936, 'word': 'patients', 'start': 143, 'end': 151}, {'entity_group': 'cond', 'score': 0.524707, 'word': 'visual hallucinations', 'start': 169, 'end': 190}, {'entity_group': 'cond', 'score': 0.70852387, 'word': \"dementia associated with parkinson ' s disease ( pdd )\", 'start': 212, 'end': 262}]\n"
     ]
    }
   ],
   "source": [
    "res = token_classifier(sentence, aggregation_strategy=\"first\")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
