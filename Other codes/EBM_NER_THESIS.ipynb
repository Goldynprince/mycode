{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from transformers import ElectraForPreTraining, ElectraTokenizerFast, DataCollatorForTokenClassification, AutoModelForTokenClassification\n",
    "import torch\n",
    "from preprocessing import create_datasets, read_alldocs, read_individual_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 files for participants\n"
     ]
    }
   ],
   "source": [
    "path1 = \"../ebm_nlp_2_00/documents/*.tokens\"\n",
    "path2 = \"../ebm_nlp_2_00/annotations/aggregated/hierarchical_labels/participants/train_test_gold/*.ann\"\n",
    "path3 = \"../ebm_nlp_2_00/annotations/aggregated/hierarchical_labels/outcomes/train_test_gold/*.ann\"\n",
    "path4 = \"../ebm_nlp_2_00/annotations/aggregated/hierarchical_labels/interventions/train_test_gold/*.ann\"\n",
    "\n",
    "participant_labels = ['O', 'PAR_AGE', 'PAR_SEX', 'PAR_SAMP_SIZE', 'PAR_COND']\n",
    "new_label = ['O', 'PAR_AGE', 'PAR_SEX', 'PAR_SAMP_SIZE', 'PAR_COND', \n",
    "             'INT_SURG','INT_PHYS', 'INT_DRUG', 'INT_EDU', 'INT_PSY', 'INT_OTHER', \n",
    "              'OUT_PHYS', 'OUT_PAIN', 'OUT_MORT', 'OUT_ADV_EFF', 'OUT_MENT', 'OUT_OTHER']\n",
    "\n",
    "partcipant_data = read_individual_docs('participants')\n",
    "# all_doc = read_alldocs(path1, path2, path3, path4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ef30a466d34b8a9995fd3695b50bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/4798 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "partcipant_ds = create_datasets(partcipant_data, participant_labels)\n",
    "# all_doc_ds = create_datasets(all_doc, new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pmid': Value(dtype='string', id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'PAR_AGE', 'PAR_SEX', 'PAR_SAMP_SIZE', 'PAR_COND'], id=None), length=-1, id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partcipant_ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import train_test_split\n",
    "part_train_test = train_test_split(partcipant_ds, train_test_size=0.1, validation_size=0.5) \n",
    "# all_doc_train_test = train_test_split(all_doc_ds, train_test_size=0.1, validation_size=0.5) \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pmid', 'ner_tags', 'tokens'],\n",
       "        num_rows: 4318\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pmid', 'ner_tags', 'tokens'],\n",
       "        num_rows: 240\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['pmid', 'ner_tags', 'tokens'],\n",
       "        num_rows: 240\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pmid', 'tokens', 'ner_tags'],\n",
       "        num_rows: 4176\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pmid', 'tokens', 'ner_tags'],\n",
       "        num_rows: 233\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['pmid', 'tokens', 'ner_tags'],\n",
       "        num_rows: 232\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'PAR_AGE', 'PAR_SEX', 'PAR_SAMP_SIZE', 'PAR_COND']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_train_test['train'].features['ner_tags'].feature.names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ElectraTokenizerFast.from_pretrained(\"kamalkraj/BioELECTRA-PICO\", model_max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_align_labels(example, label_all_tokens = True):\n",
    "    # tokenized_input = tokenizer(example['tokens'], truncation = True, is_split_into_words=True, max_length = 450)\n",
    "    tokenized_input = tokenizer(example['tokens'], truncation = True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i , label in enumerate(example['ner_tags']):\n",
    "        word_ids = tokenized_input.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None: \n",
    "                label_ids.append(-100)\n",
    "            elif word_ids!= previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else: \n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100) \n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_input['labels'] = labels\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ncbi_disease = load_dataset('ncbi_disease')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['Identification',\n",
       "  'of',\n",
       "  'APC2',\n",
       "  ',',\n",
       "  'a',\n",
       "  'homologue',\n",
       "  'of',\n",
       "  'the',\n",
       "  'adenomatous',\n",
       "  'polyposis',\n",
       "  'coli',\n",
       "  'tumour',\n",
       "  'suppressor',\n",
       "  '.'],\n",
       " 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncbi_disease['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pmid', 'ner_tags', 'tokens'],\n",
       "        num_rows: 4318\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pmid', 'ner_tags', 'tokens'],\n",
       "        num_rows: 240\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['pmid', 'ner_tags', 'tokens'],\n",
       "        num_rows: 240\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e151f88b6a4ffab57690fcfe670aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5433 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f829c45bd1447c4ba5e2abac89a8273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/924 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30e7925b52d4d8bb7b67e0e1cf5e807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/941 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = ncbi_disease.map(tokenize_align_labels, batched = True, remove_columns=ncbi_disease['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5433\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 924\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 941\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_list = ncbi_disease['train'].features['ner_tags'].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dis_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gbadamosi\\Documents\\Nerd Corner\\Master in ds and AI\\MSC project\\workspace\\personal_workspace\\mycode\\EBM_NER_THESIS.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Gbadamosi/Documents/Nerd%20Corner/Master%20in%20ds%20and%20AI/MSC%20project/workspace/personal_workspace/mycode/EBM_NER_THESIS.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m id2label \u001b[39m=\u001b[39m {i:label \u001b[39mfor\u001b[39;00m i, label \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dis_list) }\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gbadamosi/Documents/Nerd%20Corner/Master%20in%20ds%20and%20AI/MSC%20project/workspace/personal_workspace/mycode/EBM_NER_THESIS.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m label2id \u001b[39m=\u001b[39m {label:i \u001b[39mfor\u001b[39;00m i, label \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dis_list) }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dis_list' is not defined"
     ]
    }
   ],
   "source": [
    "id2label = {i:label for i, label in enumerate(dis_list) }\n",
    "label2id = {label:i for i, label in enumerate(dis_list) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O', 1: 'B-Disease', 2: 'I-Disease'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at kamalkraj/BioELECTRA-PICO and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([4, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"kamalkraj/BioELECTRA-PICO\", ignore_mismatched_sizes=True, id2label = id2label )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    'ncbi_bencmark',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_eval_batch_size=8,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = evaluate.load('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ncbi_disease['train'].features['ner_tags'].feature.names\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    pred_logits, labels = eval_preds\n",
    "\n",
    "    pred_logits= np.argmax(pred_logits, axis = 2)\n",
    "\n",
    "    prediction = [\n",
    "                [label_list[eval_preds] for (eval_preds, l ) in zip(prediction, label) if l != -100 ] for prediction, label in zip(pred_logits, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "                [label_list[l] for (eval_preds, l ) in zip(prediction, label)  if l != -100 ] for prediction, label in zip(pred_logits, labels)] \n",
    "\n",
    "    results = metric.compute(predictions= prediction, references = true_labels)\n",
    "    return{\n",
    "    'precision':results['overall_precision'],\n",
    "    'recall':results['overall_recall'],\n",
    "    'f1':results['overall_f1'],\n",
    "    'accuracy':results['overall_accuracy']\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5433\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 924\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 941\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model, \n",
    "    args,\n",
    "    train_dataset = tokenized_dataset['train'],\n",
    "    eval_dataset = tokenized_dataset['validation'],\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9974967c324c4f9f9aed6937b02f7fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1194, 'learning_rate': 5.294117647058824e-06, 'epoch': 0.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984347b929204bada42b85d332717407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04829097166657448, 'eval_precision': 0.8051001821493625, 'eval_recall': 0.8557599225556631, 'eval_f1': 0.8296574378226186, 'eval_accuracy': 0.9837176690557806, 'eval_runtime': 25.4964, 'eval_samples_per_second': 36.24, 'eval_steps_per_second': 4.55, 'epoch': 1.0}\n",
      "{'train_runtime': 641.6782, 'train_samples_per_second': 8.467, 'train_steps_per_second': 1.06, 'train_loss': 0.10320049173691694, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('EBM_tokenizer_2.0\\\\tokenizer_config.json',\n",
       " 'EBM_tokenizer_2.0\\\\special_tokens_map.json',\n",
       " 'EBM_tokenizer_2.0\\\\vocab.txt',\n",
       " 'EBM_tokenizer_2.0\\\\added_tokens.json',\n",
       " 'EBM_tokenizer_2.0\\\\tokenizer.json')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('EBM_ner_model_2.0')\n",
    "tokenizer.save_pretrained('EBM_tokenizer_2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"caching_allocator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.reset_accumulated_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1050'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4930233344"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5289017344"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_reserved()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gbadamosi\\Documents\\Nerd Corner\\Master in ds and AI\\MSC project\\workspace\\workspace\\Lib\\site-packages\\torch\\cuda\\memory.py:444: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5289017344"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_cached()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PAR_COND',\n",
       "  'score': 0.6910038,\n",
       "  'word': 'patients with and without visual hallucinations',\n",
       "  'start': 143,\n",
       "  'end': 190},\n",
       " {'entity_group': 'PAR_COND',\n",
       "  'score': 0.7989797,\n",
       "  'word': \"dementia associated with parkinson ' s disease ( pdd )\",\n",
       "  'start': 212,\n",
       "  'end': 262}]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "checkpoint = 'C:/Users/Gbadamosi/Documents/Nerd Corner/Master in ds and AI/MSC project/workspace/workspace/mycode/ebm_test_ner/checkpoint-1887'\n",
    "token_classifer = pipeline('ner', model = checkpoint, aggregation_strategy = 'simple')\n",
    "\n",
    "\n",
    "token_classifer(\"We aimed to determine prospectively whether rivastigmine, an inhibitor of acetylcholinesterase and butyrylcholinesterase, provided benefits in patients with and without visual hallucinations in a population with dementia associated with Parkinson's disease (PDD)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
